{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_maze  # noqa: F401\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pathlib\n",
    "import random\n",
    "from itertools import groupby\n",
    "\n",
    "from myst_nb import glue\n",
    "\n",
    "from src.decorators import get_from_cache_or_run\n",
    "from lcs import Perception\n",
    "\n",
    "from src.visualization import PLOT_DPI\n",
    "from src.utils import build_plots_dir_path, build_cache_dir_path\n",
    "\n",
    "root_dir = pathlib.Path().cwd().parent.parent.parent\n",
    "cwd_dir_name = pathlib.Path().cwd().name\n",
    "\n",
    "plot_dir = build_plots_dir_path(root_dir) / cwd_dir_name\n",
    "cache_dir = build_cache_dir_path(root_dir) / cwd_dir_name\n",
    "\n",
    "plt.ioff();  # turn off interactive plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Road towards Anticipatory Learning Classifier Systems\n",
    "The concept of LCS was introduced by John Holland in 1975 {cite}`holland1975adaptation` in order to model the idea of cognition based on adaptive mechanisms. From the early days they consist of a set of rules named _classifiers_  combined with mechanisms in charge of evolving them. Initially the goal was to handle problems related to online interaction with external environments, as described by Wilson in {cite}`wilson1986knowledge`.\n",
    "\n",
    "To accomplish this, the emphasis was put on parallelism in the architecture and evolutionary mechanisms allowing the agent to adapt to potentially changing environment {cite}`goldberg1988genetic`. This approach was referenced as _\"escaping brittleness\"_ {cite}`holland1986escaping` due to the problems related to the lack of robustness of the current artificial intelligence systems.\n",
    "\n",
    "`````{margin}\n",
    "````{admonition} John Henry Holland\n",
    "```{image} ../../_static/John_Holland.jpg\n",
    "```\n",
    "\n",
    "[John H. Holland](https://en.wikipedia.org/wiki/John_Henry_Holland) is best known for his role as a founding father of the complex systems approach. In particular, he developed genetic algorithms and learning classifier systems.\n",
    "\n",
    "These foundational building blocks of an evolutionary approach to optimization are now included in all texts on optimization and programming.\n",
    "````\n",
    "`````\n",
    "\n",
    "The naming convention used to refer to the LCS algorithm also changed since its infancy. Holland initially called it as a classifier system, abbreviated either as (CS) or (CFS) {cite}`robertson1988tale`. From that time it was also referred to as adaptive agents {cite}`holland1996hidden`, cognitive systems {cite}`holland1978cognitive` and genetic-based machine learning {cite}`dam2007neural` {cite}`goldberg1989`. The current name of LCS was not adopted until the late 80s {cite}`riolo1988empirical` after extending the architecture with a credit assignment component {cite}`holland1986escaping` {cite}`holland1986mathematical`.\n",
    "\n",
    "This section provides a synopsis of crucial LCS concepts alongside the most popular variants, and the contributions they made to the field.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Selected topics of LCS\n",
    "### Rules and Classifiers\n",
    "\n",
    "In general form, LCS utilizes _rules_ as a fundamental block of modeling knowledge. It comprises a _condition_ (i.e. specified feature states), and an _action_ (also referred to as the class). They can be interpreted using the \"IF condition THEN action\" logical expression. When the condition part is expressed with either boolean or nominal representation the generalization property using the \"don't care\" symbol # is possible. Two input situations are considered equivalent with respect to a given classifier if the specified (non-#) values in the condition match the corresponding attributes of the two situations\n",
    "\n",
    "In addition to condition and action, a rule typically has a number of algorithm related parameter values associated with it (like its performance or expected reward). The term _classifier_ is used to describe a rule along with it associated parameters.\n",
    "\n",
    "It is important to realise that LCS comprise a population of single rules, that collaboratively seek to cover the problem space. The number of classifiers needed to solve the particular problem depends on a number of factors like problem complexity or rule representation used."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Driving Mechanisms\n",
    "There are two fundamental components behind every LCS algorithm - the discovery and learning (credit assignment) component. Both of them have generated respective fields of study, but it is in the context of LCS that we wish to understand their function and purpose.\n",
    "\n",
    "**Discovery component**\n",
    "\n",
    "The discovery component is responsible for exploring the search space of the target problem with the purpose of discovering new rules. A vast majority of LCS algorithms employ some form of Evolutionary Computation, most often Genetic Algorithm (GA), employing Neo-Darwinist theory of natural selection {cite}`bull2005foundations` {cite}`goldberg1989`. The evolution of rules is modeled after the evolution of organisms using following biological analogies:\n",
    "\n",
    "1. a code is used to represent the genotype/genome (condition),\n",
    "2. a solution (phenotype) representation is associated with that genome (action),\n",
    "3. a phenotype selection process (survival of the fittest) - the fittest organism (rule) has the greatest chances of reproducing and passing parts of its genome to offsprings,\n",
    "4. certain genetic genome operators are utilized in order to chase after fitter organisms (rules) {cite}`sigaud2007learning` {cite}`holmes1999learning`.\n",
    "\n",
    "Two genetic operators are typically used to alternate genome (rule) - mutation and crossover (recombination). The first one randomly modify an element in a genotype of an individual (rule), while the latter one recombine parts of two promising genotypes (rules) creating a new one. The selection pressure driving better organisms (rules) to reproduce more frequently depends on the fitness function. The fitness function quantifies the optimality of given rule, allowing it to be ranked across all rules in entire population.\n",
    "\n",
    "GA implementation varies in certain LCS, but the overall scheme involves evaluating all available rules, selecting most promising offsprings (according to fitness value), applying genetic operators, introducing new offspring back to population set and finally removing excess or under-performing individuals.\n",
    "\n",
    "\n",
    "**Learning component**\n",
    "\n",
    "As mentioned earlier each classifier is accompanied by certain parameter values. The iterative update of them drives the process of LCS reinforcement by distributing any incoming reward signal to the classifiers that are accountable for it. This process serves two purposes:\n",
    "\n",
    "1. identification of classifiers responsible for obtaining large future rewards,\n",
    "2. encourage the discoverability of new rules (by directly affecting the fitness value)\n",
    "\n",
    "The learning strategy depends on the nature of the problem and is realized differently in LCS implementations. In all cases, however the process is performed through trial-and-error interactions with the environment, where occasional immediate reward is used to generate the policy (state-action mapping of agent-environment interactions) maximizing long-term reward {cite}`sutton2018reinforcement` {cite}`harmon96reinforcementlearning` {cite}`richter2003reinforcement`.\n",
    "\n",
    "\n",
    "### Functional cycle\n",
    "The agent interacts with the environment in consecutive trials. Each trial consists of sequential steps usually executed as follows:\n",
    "\n",
    "1. Filter population $[P]$ and select classifiers where condition matches environmental perception forming a _match-set_ $[M]$.\n",
    "2. Determine action that will be executed (depending on the strategy).\n",
    "3. Narrow-down the match-set by selecting only classifiers advocating proposed action - create the _action-set_ $[A]$.\n",
    "4. Execute the action in the environment obtaining a new state\n",
    "5. Refine classifiers by executing discovery and learning components.\n",
    "\n",
    "The action selection and classifiers evolution phases are implemented individually in different LCS, but the main objective of reaching ideal generalization level is crucial to all of them. The system should find a population that covers the search space as compactly as possible without being detrimental to the optimality of behavior.\n",
    "\n",
    "### Pittsburgh vs Michigan\n",
    "One of the most fundamental distinction in LSC research is the way of storing knowledge by using two different approaches - _Michigan-style_ or _Pittsburgh-style_. The first ones were proposed by Holland {cite}`holland1978cognitive` while the latter one by Kenneth DeJong and his student {cite}`smith1980learning` {cite}`smith1983flexible`.\n",
    "\n",
    "The fundamental distinction is the structure of an individual. In Michigan systems each individual is a classifier, in Pittsburgh each individual is a set of classifiers - see Figure {numref}`{number} <michigan-pittsburgh-fig>`.\n",
    "\n",
    ":::{figure-md} michigan-pittsburgh-fig\n",
    "<img src=\"../../_static/michigan_pittsburgh.png\">\n",
    "\n",
    "Differences between representing knowledge in both Michigan-style and Pittsburgh-style LCS. Figure taken from {cite}`bacardit2006learning`.\n",
    ":::\n",
    "\n",
    "Therefore, the classifiers in Michigan-style LCS are being continuously evaluated and evolved, while in Pittsburgh-style LCS the same process is much more complicated because the whole population needs to be assessed. Thus, Michigan-style systems are typically applied in interactive, online learning problems, while Pittsburgh ones are rather suitable for offline learning {cite}`bacardit2006learning`.\n",
    "\n",
    "This work focuses solely on the first class of LCS."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Representative LCS\n",
    "This sections describes Michigan-style LCS implementations contributing mostly to the current advancements."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CS-1\n",
    "Two years after presenting the theoretical model of CS Holland and Reitman proposed its implementation CS-1 {cite}`holland1978cognitive`. The system realized the Darwinian principle of the _survival of the fittest_  {cite}`holland1989induction` and was generating adaptive behaviour by maximizing reinforcement using the bucket brigade algorithm (BBA) {cite}`samuel2000some` {cite}`holland1985properties`.\n",
    "\n",
    "Some internal mechanisms like the usage of an internal message list (handling all input and output communications between the system and the environment and providing a makeshift memory) or interactions between evolving both single classifier and entire population were considered difficult {cite}`wilson1989critical` {cite}`goldberg1989`. Moreover, the obtained results were inconsistent which already signified the need for improvements.\n",
    "\n",
    "`````{margin}\n",
    "````{admonition} Steward Wilson\n",
    "```{image} ../../_static/steward_wilson.gif\n",
    "```\n",
    "\n",
    "[Steward Wilson](https://ieeexplore.ieee.org/author/37550361200) is an Independent Researcher in classifier systems and evolutionary computation.\n",
    "\n",
    "His Ph.D. thesis investigated what would happen if a child could ask questions and receive excellent spoken answers from a machine that connected him to recordings made by a scientist such as Carl Sagan. The results showed long, thoughtful lines of inquiry for this student-driven style of learning.\n",
    "````\n",
    "`````\n",
    "### ZCS\n",
    "The _Zeroth Level Classifier_ (ZCS) introduced by Wilson in 1994 {cite}`wilson1994zcs` encompasses all LCS components while simplifying the CS-1 increasing its understandability and performance. The major changes was the removing of the internal message list (determining the rule's format entirely by the system interface) alongside with rule-bidding credit assignment (replacing it with BBA/Q-learning {cite}`watkins1989learning` algorithm).\n",
    "\n",
    "Moreover, the classifier fitness was based on the accumulated reward that the agent can get from firing the classifier which gave rise to the _\"strength-based\"_ family of LCS. As a result the discovery component eliminates classifiers providing less reward than others from the population.\n",
    "\n",
    "ZCS managed to achieve similar performance to CS-1, demonstrating Holland's idea could work even in a very simple framework. However, the premature converge onto suboptimal rules before search space can be properly explored and stable population formed led Wilson to consider other ways in which this might be achieved.\n",
    "\n",
    "(section-topics-history-xcs)=\n",
    "### XCS\n",
    "In 1995 Wilson introduce yet another, groundbreaking modification called the _eXtended Classifier System_ (XCS) {cite}`wilson1995classifier` {cite}`wilson1998generalization` {cite}`wilson1999state` {cite}`butz2000algorithmic` noted for being able to reach optimal performance while evolving accurate and maximally general classifiers.\n",
    "\n",
    "The most important changes includes:\n",
    "\n",
    "- rule fitness is based on accuracy of predictions (forming the _accuracy-based_ LCS family),\n",
    "- replacement of panmictically acting GA with a _niche-GA_  {cite}`booker1985improving` (acting only in action set $[A]$ instead of globally $[P]$),\n",
    "- explicit generalization mechanism (_subsumption_),\n",
    "- adaptation of Q-Learning credit assignment\n",
    "\n",
    "The XCS design drives it to form an all-inclusive and accurate representation of the problem space rather than focusing on higher payoff niches. Very promising performance results showed that RL {cite}`sigaud2007learning` and LCS are not only linked but inherently overlapping, redefining LCSs as RL endowed with a generalization capabilities {cite}`lanzi2005learning` {cite}`lanzi2008learning`. As a result it becomes the most popular LCS implementation, guiding other system implementations that are heavily inspired by its architecture."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Anticipatory Learning Classifier Systems\n",
    "The investigation about the presence and importance of anticipations were initially guided by the field of cognitive psychology. It was proved that \"higher\" animals form and exploit anticipations while adopting their behaviour in distinct tasks. Wilson noted that when simulating adaptive behaviour the animats should be endowed with anticipations {cite}`wilson1991animat`. In traditional RL the first approaches manifested in Dyna architecture {cite}`sutton2018reinforcement`, but due to the lack of generalization capabilities its utilization was limited. The Anticipatory Learning Classifier Systems (ALCS) aims to bridge the gap for exploiting the generalization capabilities while including the explicit notion of anticipations.\n",
    "\n",
    "### CFCS2\n",
    "````{margin}\n",
    "```{admonition} Latent learning\n",
    "Learning without an explicit reward feedback (e.g. learning a sequence of states based on actions taken and the following expected states).\n",
    "```\n",
    "````\n",
    "First foundations towards including predictions were laid by Riolo in 1991 by introducing a modification called CFCS2 {cite}`riolo1991lookahead`. It addressed the task of performing \"latent learning\" or \"lookahead planning\" where _\"actions are based on predictions of future states of the world, using both current information and past experience as embodied in the agent's internal models of the world\"_ {cite}`lanzi2003recent`.\n",
    "\n",
    "CFCS2 used \"tags\" to specify if a current action posted to a message list refers to an actual action or an anticipation, claiming a reduction in the learning time for general sequential decision tasks. Riolo was able to show the possibilities of latent learning and lookahead planning, but the implicit formation of anticipations appeared to be misleading. Additionally, the CFCS2 did not achieve any generalization capabilities.\n",
    "\n",
    "### ACS\n",
    "The first system capable of evolving a maximally generalized, accurate and complete mapping of all possible situation-effect-action triples observable in the environment - the Anticipatory Classifier System (ACS) was introduced by Stolzmann in 1997 {cite}`stolzmann1997antizipative` {cite}`stolzmann1999introduction`.\n",
    "\n",
    "The rule structure was enhanced with an anticipatory or _effect_ part that anticipates the effects of an action in a given situation. The effect part was capable of determining which attributes change after executing an action by using a \"pass-through\" symbol - $\\#$, which greatly enhances the rule's interpretability. In order to perform latent learning, forming and refining classifiers a new discovery component - the Anticipatory Learning Process (ALP) was introduced.\n",
    "\n",
    "````{margin}\n",
    "```{admonition} Perceptual aliasing\n",
    "Case when same observation is obtained in distinct states requiring different actions.\n",
    "```\n",
    "````\n",
    "ALP realizes the psychological Hoffmann's anticipatory behavioral control theory {cite}`herbart1825psychologie` {cite}`hoffmann1993vorhersage` stating that conditional action-effects relations are learned latently using anticipations, which he further refined in {cite}`hoffmann2000lernmechanismen`. The following points (visualised in Figure {numref}`{number} <abc-fig>`) can be distinguished:\n",
    "\n",
    "1. Any behavioural act or response ($R$) is accompanied by anticipation of its effects.\n",
    "2. The anticipations of the effects $E_{ant}$ are compared with the real effects $E_{real}$.\n",
    "3. When the anticipations were correct, the bond between response and anticipation is strengthened and weakened otherwise.\n",
    "4. Behavioural stimuli further differentiate the $R-E_{ant}$ relations.\n",
    "\n",
    "```{figure} ../../_static/abc.png\n",
    "---\n",
    "name: abc-fig\n",
    "---\n",
    "\n",
    "The theory of anticipatory behavioural control. Adapted from {cite}`hoffmann2000lernmechanismen`.\n",
    "```\n",
    "\n",
    "The proposed architecture was capable of solving multistep problems, planning, speeding up learning or disambiguating perceptual aliasing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/papermill.record/text/plain": "None"
     },
     "metadata": {
      "scrapbook": {
       "name": "acs2_policy_fig",
       "mime_prefix": "application/papermill.record/"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lcs.agents.acs2 as acs2\n",
    "from lcs.metrics import population_metrics\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "def maze_metrics(agent, env):\n",
    "    metrics = {}\n",
    "    metrics.update(population_metrics(agent.population, env))\n",
    "    return metrics\n",
    "\n",
    "acs2_base_params = {\n",
    "    'classifier_length': 8,\n",
    "    'number_of_possible_actions': 8,\n",
    "    'biased_exploration': 0,\n",
    "    'metrics_trial_frequency': 1,\n",
    "    'user_metrics_collector_fcn': maze_metrics\n",
    "}\n",
    "\n",
    "def run_acs2_explore_exploit(env, explore_trials, exploit_trials, **config):\n",
    "    cfg = acs2.Configuration(**config)\n",
    "\n",
    "    # explore phase\n",
    "    agent = acs2.ACS2(cfg)\n",
    "    metrics_explore = agent.explore(env, explore_trials)\n",
    "\n",
    "    # exploit phase\n",
    "    agent_exploit = acs2.ACS2(cfg, copy(agent.population))\n",
    "    metrics_exploit = agent_exploit.exploit(env, exploit_trials)\n",
    "\n",
    "    return (agent, metrics_explore), (agent_exploit, metrics_exploit)\n",
    "\n",
    "def find_best_classifier(population: acs2.ClassifiersList, situation: Perception) -> Optional[acs2.Classifier]:\n",
    "    match_set = population.form_match_set(situation)\n",
    "    anticipated_change_cls = [cl for cl in match_set if cl.does_anticipate_change()]\n",
    "\n",
    "    if len(anticipated_change_cls) > 0:\n",
    "        return max(anticipated_change_cls, key=lambda cl: cl.fitness)\n",
    "\n",
    "    return None\n",
    "\n",
    "def build_fitness_and_action_matrices(env, population) -> Tuple:\n",
    "    original = env.env.maze.matrix\n",
    "\n",
    "    fitness = original.copy()\n",
    "    action = original.copy().astype(str)\n",
    "\n",
    "    action_lookup = {\n",
    "        0: u'↑', 1: u'↗', 2: u'→', 3: u'↘',\n",
    "        4: u'↓', 5: u'↙', 6: u'←', 7: u'↖'\n",
    "    }\n",
    "\n",
    "    for index, x in np.ndenumerate(original):\n",
    "        if x == 0:  # path\n",
    "            perception = env.env.maze.perception(index)\n",
    "            best_cl = find_best_classifier(population, perception)\n",
    "\n",
    "            if best_cl:\n",
    "                fitness[index] = best_cl.fitness\n",
    "                action[index] = action_lookup[best_cl.action]\n",
    "            else:\n",
    "                fitness[index] = -1\n",
    "                action[index] = '?'\n",
    "\n",
    "        if x == 1:  # wall\n",
    "            fitness[index] = 0\n",
    "            action[index] = '\\#'\n",
    "\n",
    "        if x == 9:  # reward\n",
    "            # add 500 to make it more distinguishable\n",
    "            fitness[index] = fitness.max() + 500\n",
    "            action[index] = 'R'\n",
    "\n",
    "    return fitness, action\n",
    "\n",
    "def plot_policy(env, fitness_matrix, action_matrix, plot_filename=None):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "\n",
    "    max_x, max_y = env.env.maze.matrix.shape\n",
    "\n",
    "    # Render maze as image\n",
    "    plt.imshow(fitness_matrix, interpolation='nearest', cmap='Reds', aspect='auto', extent=[0, max_x, max_y, 0])\n",
    "\n",
    "    # Add labels to each cell\n",
    "    for (y, x), val in np.ndenumerate(action_matrix):\n",
    "        plt.text(x + 0.4, y + 0.5, \"${}$\".format(val))\n",
    "\n",
    "    ax.set_title(\"ACS2 policy in Maze5 environment\", fontsize=24)\n",
    "    ax.set_xlabel('x', fontsize=18)\n",
    "    ax.set_ylabel('y', fontsize=18)\n",
    "    ax.set_xlim(0, max_x)\n",
    "    ax.set_ylim(max_y, 0)\n",
    "    ax.set_xticks(range(0, max_x))\n",
    "    ax.set_yticks(range(0, max_y))\n",
    "    ax.grid(True)\n",
    "\n",
    "    if plot_filename:\n",
    "        fig.savefig(plot_filename, dpi=PLOT_DPI)\n",
    "\n",
    "    return fig\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/acs2_maze5.dill')\n",
    "def run_acs2_in_maze5():\n",
    "    env = gym.make('Maze5-v0')\n",
    "    explore_phase, exploit_phase = run_acs2_explore_exploit(env, explore_trials=5000, exploit_trials=100, **acs2_base_params)\n",
    "    return env, explore_phase, exploit_phase\n",
    "\n",
    "# Run computation\n",
    "env_, explore_, exploit_ = run_acs2_in_maze5()\n",
    "\n",
    "# Plot the policy\n",
    "fitness_matrix, action_matrix = build_fitness_and_action_matrices(env_, explore_[0].population)\n",
    "acs2_policy_fig = plot_policy(env_, fitness_matrix, action_matrix, plot_filename=f'{plot_dir}/acs2-maze5-policy.png')\n",
    "\n",
    "glue('acs2_policy_fig', acs2_policy_fig, display=False)\n",
    "glue('acs2_population_size', len(explore_[0].population), display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ACS2\n",
    "Later,  in  2002 Martin Butz  {cite}`butz2002anticipatory` {cite}`butz2001algorithmic`  presented  an  extension  called ACS2.  The  significant  changes  from  the  previous  version include\n",
    "\n",
    "- application of learning component across the whole action set $[A]$ (all classifiers from $[M]$ advocating selected action),\n",
    "- introduction of GA component,\n",
    "- condition-action-effect triples that anticipate no change in the environment are encompassed in population\n",
    "\n",
    "Experiments showed that the genetic generalization process in ACS2 was able to decrease the population size and consequently speed up the computational process.\n",
    "\n",
    "Figure {numref}`{number} <acs2_policy_fig>` shows an example of RL policy generated by a population of {glue:}`acs2_population_size` classifiers trained over 5000 trials operating in popular Maze5 benchmarking problem. The model knows the consequences of each available action in every possible state, therefore after estimating the fitness value of each classifier is able to suggest most promising action. The learning can be still optimized by using cognitive mechanisms like the lookahead planning.\n",
    "\n",
    "```{glue:figure} acs2_policy_fig\n",
    ":name: \"acs2_policy_fig\"\n",
    "\n",
    "Policy generated by ACS2 in Maze5 environment. Saturation of red color reflects the best classifier fitness value. In each trial the agent is inserted randomly on the red field with the goal of reaching the reward state \"R\" by executing eight possible actions.\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### YACS\n",
    "Gérard introduced a “_Yet Another Classifier System_” (YACS) {cite}`gerard2002yacs` in 2002. It shares the same C-A-E classifier structure as ACS and ACS2 but the essential conceptual difference between those two is that YACS is designed to decorrelate the acquisition of relevant $C$ and $E$ parts by building them independently using a set of heuristics. The latent learning process is designed to set the E parts to actually perceived changes in the environment and then to discover relevant C parts. Additionally, it does not take advantage of genetic generalization mechanisms relying on the discovery process to correctly determine significant attributes.\n",
    "\n",
    "Evaluation was performed in simple Maze environment reaching _near optimal_ solutions. The specialization process in YACS leads to less over-specialization then the corresponding process in ACS, but still suffers from the lack of dedicated generalization mechanism."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[30m■\u001B[0m \u001B[30m■\u001B[0m \u001B[30m■\u001B[0m \u001B[30m■\u001B[0m \u001B[30m■\u001B[0m \u001B[30m■\u001B[0m \u001B[30m■\u001B[0m\n",
      "\u001B[30m■\u001B[0m \u001B[37m□\u001B[0m \u001B[30m■\u001B[0m \u001B[37m□\u001B[0m \u001B[30m■\u001B[0m \u001B[37m□\u001B[0m \u001B[30m■\u001B[0m\n",
      "\u001B[30m■\u001B[0m \u001B[37m□\u001B[0m \u001B[37m□\u001B[0m \u001B[37m□\u001B[0m \u001B[37m□\u001B[0m \u001B[37m□\u001B[0m \u001B[30m■\u001B[0m\n",
      "\u001B[30m■\u001B[0m \u001B[37m□\u001B[0m \u001B[30m■\u001B[0m \u001B[30m■\u001B[0m \u001B[37m□\u001B[0m \u001B[37m□\u001B[0m \u001B[30m■\u001B[0m\n",
      "\u001B[30m■\u001B[0m \u001B[37m□\u001B[0m \u001B[30m■\u001B[0m \u001B[37m□\u001B[0m \u001B[37m□\u001B[0m \u001B[37m□\u001B[0m \u001B[30m■\u001B[0m\n",
      "\u001B[30m■\u001B[0m \u001B[37m□\u001B[0m \u001B[37m□\u001B[0m \u001B[37m□\u001B[0m \u001B[31mA\u001B[0m \u001B[33m$\u001B[0m \u001B[30m■\u001B[0m\n",
      "\u001B[30m■\u001B[0m \u001B[30m■\u001B[0m \u001B[30m■\u001B[0m \u001B[30m■\u001B[0m \u001B[30m■\u001B[0m \u001B[30m■\u001B[0m \u001B[30m■\u001B[0m\n",
      "Perception: 009111000\n"
     ]
    }
   ],
   "source": [
    "import lcs.agents.macs.macs as macs\n",
    "\n",
    "maze228_env = gym.make('Maze228-v0')\n",
    "\n",
    "def _calculate_rotating_maze_knowledge(agent: macs.MACS, env):\n",
    "    transitions = env.env.transitions\n",
    "    covered_transitions = 0\n",
    "\n",
    "    for p0, a, p1 in transitions:\n",
    "        p0p = Perception(list(map(str, p0)))\n",
    "        p1p = Perception(list(map(str, p1)))\n",
    "        anticipations = list(agent.get_anticipations(p0p, a))\n",
    "\n",
    "        # accurate classifiers\n",
    "        if len(anticipations) == 1 and anticipations[0] == p1p:\n",
    "            covered_transitions += 1\n",
    "\n",
    "    return covered_transitions / len(transitions)\n",
    "\n",
    "def _macs_metrics(agent: macs.MACS, env):\n",
    "    population = agent.population\n",
    "    return {\n",
    "        'pop': len(population),\n",
    "        'situations': len(agent.desirability_values),\n",
    "        '0_cls': len([cl for cl in population if cl.action == 0 and cl.is_accurate]),\n",
    "        '1_cls': len([cl for cl in population if cl.action == 1 and cl.is_accurate]),\n",
    "        '2_cls': len([cl for cl in population if cl.action == 2 and cl.is_accurate]),\n",
    "        'knowledge': _calculate_rotating_maze_knowledge(agent, env)\n",
    "    }\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/macs_maze228.dill')\n",
    "def run_macs_in_maze228(env):\n",
    "    cfg = macs.Configuration(classifier_length=9,\n",
    "                        number_of_possible_actions=3,\n",
    "                        feature_possible_values=[{'0', '1', '9'}] * 8 + [{'0', '9'}],\n",
    "                        estimate_expected_improvements=True,\n",
    "                        metrics_trial_frequency=10,\n",
    "                        user_metrics_collector_fcn=_macs_metrics)\n",
    "\n",
    "    agent = macs.MACS(cfg)\n",
    "    metrics = agent.explore(env, 100)\n",
    "\n",
    "    return agent, metrics\n",
    "\n",
    "# run computations\n",
    "macs_agent, macs_metrics = run_macs_in_maze228(maze228_env)\n",
    "\n",
    "# visualization\n",
    "random.seed(129)  # found experimentally\n",
    "maze228_env.reset()\n",
    "maze228_env.render()\n",
    "\n",
    "left_from_reward = maze228_env.env.maze.perception()\n",
    "\n",
    "print(f'Perception: {\"\".join(left_from_reward)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total MACS population size: 211\n",
      "\n",
      "Action: STEP_AHEAD, anticipation: [0 0 0 9 0 0 0 1 0]\n",
      "\t0#9###### 0 0????????\n",
      "\t#0###10## 0 ?0???????\n",
      "\t0#9##1### 0 ?0???????\n",
      "\t#0####### 0 ??0??????\n",
      "\t0#9###### 0 ???9?????\n",
      "\t0######## 0 ????0????\n",
      "\t0#####0## 0 ?????0???\n",
      "\t#######0# 0 ??????0??\n",
      "\t##9###### 0 ??????0??\n",
      "\t##91#10## 0 ???????1?\n",
      "\t0######## 0 ????????0\n",
      "\n",
      "Action: ROTATE_LEFT, anticipation: [0 0 0 0 9 1 1 1 0]\n",
      "\t######0## 1 0????????\n",
      "\t#######0# 1 ?0???????\n",
      "\t0######## 1 ??0??????\n",
      "\t#0####### 1 ???0?????\n",
      "\t##9###### 1 ????9????\n",
      "\t###1##### 1 ?????1???\n",
      "\t####1#### 1 ??????1??\n",
      "\t#####1### 1 ???????1?\n",
      "\t######### 1 ????????0\n",
      "\n",
      "Action: ROTATE_RIGHT, anticipation: [9 1 1 1 0 0 0 0 0]\n",
      "\t##9###### 2 9????????\n",
      "\t###1##### 2 ?1???????\n",
      "\t####1#### 2 ??1??????\n",
      "\t#####1### 2 ???1?????\n",
      "\t######0## 2 ????0????\n",
      "\t#######0# 2 ?????0???\n",
      "\t0######## 2 ??????0??\n",
      "\t#0####### 2 ???????0?\n",
      "\t######### 2 ????????0\n"
     ]
    }
   ],
   "source": [
    "print(f'Total MACS population size: {len(macs_agent.population)}')\n",
    "s = sorted(macs_agent.population.form_match_set(left_from_reward), key=lambda cl: cl.action)\n",
    "\n",
    "action_mapping = ['STEP_AHEAD', 'ROTATE_LEFT', 'ROTATE_RIGHT']\n",
    "\n",
    "for action, group_cl in groupby(s, key=lambda cl: cl.action):\n",
    "    print(f'\\nAction: {action_mapping[action]}, anticipation: {list((macs_agent.get_anticipations(left_from_reward, action)))}')\n",
    "    for cl in sorted(group_cl, key=lambda cl: cl.effect):\n",
    "        print(f'\\t{cl.condition} {cl.action} {cl.effect}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Comparing the ACS2 agent in the same environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def _acs2_metrics(agent: acs2.ACS2, env):\n",
    "    population = agent.population\n",
    "    reliable = [cl for cl in population if cl.is_reliable()]\n",
    "    return {\n",
    "        'pop': len(population),\n",
    "        'rel': len(reliable)\n",
    "    }\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/acs2_maze228.dill')\n",
    "def run_acs2_in_maze228(env):\n",
    "    cfg = acs2.Configuration(classifier_length=9,\n",
    "                        number_of_possible_actions=3,\n",
    "                        metrics_trial_frequency=1,\n",
    "                        user_metrics_collector_fcn=_acs2_metrics,\n",
    "                        do_ga=False)\n",
    "\n",
    "    agent = acs2.ACS2(cfg)\n",
    "    metrics = agent.explore(env, 5000)\n",
    "\n",
    "    return agent, metrics\n",
    "\n",
    "acs2_agent, acs2_metrics = run_acs2_in_maze228(maze228_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reliable population for ACS2: 402\n",
      "##911#00# 1 ##009#11#\n",
      "##9111#0# 0 ##0900#1#\n",
      "009#11### 2 911#00###\n"
     ]
    }
   ],
   "source": [
    "reliable_classifiers = [cl for cl in acs2_agent.population if cl.is_reliable()]\n",
    "\n",
    "print(f'Total reliable population for ACS2: {len(reliable_classifiers)}')\n",
    "for cl in acs2_agent.population.form_match_set(left_from_reward):\n",
    "    if cl.is_reliable():\n",
    "        print(f'{cl.condition} {cl.action} {cl.effect}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "\n",
    "**Software packages used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<details>\n<summary>Click to view session information</summary>\n<pre>\n-----\ngym                 0.21.0\ngym_maze            NA\nlcs                 NA\nmatplotlib          3.5.1\nmyst_nb             0.13.1\nnumpy               1.22.1\npydev_jupyter_utils NA\npydev_jupyter_vars  NA\nsession_info        1.0.0\nsrc                 (embedded book's utils module)\n-----\n</pre>\n<details>\n<summary>Click to view modules imported as dependencies</summary>\n<pre>\nPIL                 8.4.0\nasttokens           NA\nattr                21.4.0\nbabel               2.9.1\nbackcall            0.2.0\nbrotli              NA\ncertifi             2021.10.08\ncffi                1.15.0\ncharset_normalizer  2.0.10\nclick               7.1.2\ncloudpickle         2.0.0\ncolorama            0.4.4\ncolorful            0.5.4\ncolorful_orig       0.5.4\ncryptography        36.0.1\ncycler              0.10.0\ncython_runtime      NA\ndatabricks_cli      NA\ndateutil            2.8.2\ndebugpy             1.5.1\ndecorator           5.1.1\ndefusedxml          0.7.1\ndill                0.3.4\ndocutils            0.16\nentrypoints         0.3\nexecuting           0.8.2\nfilelock            3.4.2\ngoogle              NA\ngreenlet            1.1.2\ngrpc                1.43.0\nhiredis             2.0.0\nidna                3.3\nimagesize           NA\nimportlib_metadata  NA\nipykernel           6.7.0\nipython_genutils    0.2.0\nipywidgets          7.6.5\njedi                0.18.1\njinja2              3.0.3\njsonschema          3.2.0\njupyter_cache       0.4.3\njupyter_sphinx      0.3.2\njupyterlab_pygments 0.1.2\nkiwisolver          1.3.2\nlinkify_it          1.0.3\nlxml                4.7.1\nmarkdown_it         1.1.0\nmarkupsafe          2.0.1\nmatplotlib_inline   NA\nmdit_py_plugins     0.2.8\nmistune             0.8.4\nmlflow              1.23.1\nmpl_toolkits        NA\nmsgpack             1.0.3\nmyst_parser         0.15.2\nnbclient            0.5.10\nnbconvert           6.4.1\nnbformat            5.1.3\nnetworkx            2.5\npackaging           21.3\npandas              1.4.0\npandocfilters       NA\nparso               0.8.3\npexpect             4.8.0\npickleshare         0.7.5\npkg_resources       NA\nprompt_toolkit      3.0.26\npsutil              5.9.0\nptyprocess          0.7.0\npure_eval           0.2.2\npvectorc            NA\npydevconsole        NA\npydevd_file_utils   NA\npydevd_plugins      NA\npygments            2.11.2\npylab               NA\npyparsing           3.0.7\npyrsistent          NA\npytz                2021.3\nray                 1.9.2\nredis               4.1.2\nrequests            2.27.1\nscipy               1.7.3\nsetproctitle        1.2.2\nsetuptools          60.5.0\nsix                 1.16.0\nsocks               1.7.1\nsphinx              4.4.0\nsphinxcontrib       NA\nsqlalchemy          1.4.31\nstack_data          0.1.4\ntestpath            0.5.0\ntornado             6.1\ntqdm                4.62.3\ntraitlets           5.1.1\ntyping_extensions   NA\nuc_micro            1.0.1\nunicodedata2        NA\nurllib3             1.26.8\nwcwidth             0.2.5\nyaml                6.0\nzipp                NA\nzmq                 22.3.0\n</pre>\n</details> <!-- seems like this ends pre, so might as well be explicit -->\n<pre>\n-----\nIPython             8.0.1\njupyter_client      7.1.2\njupyter_core        4.9.1\nnotebook            6.4.8\n-----\nPython 3.9.10 | packaged by conda-forge | (main, Feb  1 2022, 21:24:11) [GCC 9.4.0]\nLinux-5.13.0-28-generic-x86_64-with-glibc2.31\n-----\nSession information updated at 2022-02-18 14:27\n</pre>\n</details>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import session_info\n",
    "session_info.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}