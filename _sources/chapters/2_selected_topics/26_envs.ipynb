{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, Markdown, display\n",
    "from myst_nb import glue\n",
    "from tabulate import tabulate\n",
    "\n",
    "from src.utils import build_plots_dir_path\n",
    "from src.visualization import PLOT_DPI\n",
    "\n",
    "plt.ioff()  # turn off interactive plotting\n",
    "\n",
    "root_dir = pathlib.Path().cwd().parent.parent.parent\n",
    "cwd_dir_name = pathlib.Path().cwd().name\n",
    "\n",
    "plot_dir = build_plots_dir_path(root_dir) / cwd_dir_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(section-topics-environments)=\n",
    "# Overview of the selected environments\n",
    "\n",
    "This section describes six, both single- and multi-steps  environments used in further experiments. All of them are stationary toy-problems exhibiting real-valued properties facilitating the evaluation of modifications of anticipatory systems. Most of the environments were used as benchmarks in other works related to measuring the performance of the LCS, however the [](section-topics-environments-cartpole) environment was never used in this class of algorithms before.\n",
    "\n",
    "````{margin}\n",
    "```{admonition} Stationary environment\n",
    "A stationary environment refers to data-generating distributions that **do not change over time**.\n",
    "```\n",
    "````\n",
    "\n",
    "All the mentioned environments poses the standardized interface for enabling the agent to interact with it in consistent manner. In each step they present the current observation (that is not equal to its internal state), possible reward from previously executed action and the information whether the interaction is finished. In most cases there is a certain reward state, that acts as an incentive, motivating the forager to reach it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(section-topics-environments-corridor)=\n",
    "## Corridor\n",
    "The corridor is a 1D multi-step, linear environment introduced by Lanzi to evaluate the XCSF agent {cite}`lanzi2005xcs`. The system output is defined over a finite discrete interval $[0,n]$. On each trial, the agent is placed randomly on the path and can execute two possible actions - move left or right (which corresponds to moving one unit in a particular direction - see Figure {numref}`{number} <corridor-env>`.\n",
    "\n",
    ":::{figure-md} corridor-env\n",
    "<img src=\"../../_static/corridor.png\" width=\"75%\">\n",
    "\n",
    "The _Corridor_ environment. The size of the input space is $n$.\n",
    ":::\n",
    "\n",
    "Lanzi used a real-valued version of this environment where the agent location is elucidated by a value between $[0,1]$. Predefined step size was added to the current position when it executed an action, thus changing its value. When the agent reaches the final state $s = 1.0$, the reward is paid out.\n",
    "\n",
    "The environment examined herein signifies the state already in discretized form, available in three preconfigured sizes with increasing difficulty. The main challenge for the agent here is mainly to successfully learn the reward distribution in possibly long action chains.\n",
    "\n",
    "**Reward scheme**: The trial ends when it reaches the final state $n$ (obtaining reward $r=1000$) or when the maximum number of 200 steps in each episode is exceeded. Otherwise, the reward after each step is $r=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(section-topics-environments-grid)=\n",
    "## Grid\n",
    "Grid refers to an extension of the [](section-topics-environments-corridor) environment {cite}`lanzi2005xcs`. A vertical dimension and two new actions (move up, move down) are added. The raw agent perception is now identified as a pair of real numbers $(s_0, s_1)$, where $s \\in [0,1]$. Similarly, the environment is presented to the agent in a discretized form. Each dimension is divided into $n-1$ equally spaced buckets - see Figure {numref}`{number} <grid-env>`.\n",
    "\n",
    ":::{figure-md} grid-env\n",
    "<img src=\"../../_static/grid.png\" width=\"75%\">\n",
    "\n",
    "The _Grid_ environment. The size of the input space is $n^2$. The agent can change its position by moving in four different directions.\n",
    ":::\n",
    "\n",
    "**Reward scheme**: The trial ends with reward $r=1000$ when the final state $(n, n)$ is reached. Otherwise, the reward after each step is $r=0$. Additionally, the episode is terminated after exceeding the maximum number of 2000 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(section-topics-environments-rmpx)=\n",
    "## Real Multiplexer (rMPX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import gym_multiplexer  # noqa: F401\n",
    "\n",
    "rmpx6bit_env = gym.make('real-multiplexer-6bit-v0')\n",
    "random_state = rmpx6bit_env.reset()\n",
    "\n",
    "threshold = 0.5  # secret knowledge\n",
    "\n",
    "t = HTML(tabulate([\n",
    "    ['<b>rMPX signal</b>'] + [round(s, 2) for s in random_state],\n",
    "    ['<b>MPX signal </b>'] + list(map(lambda x: 1 if x > threshold else 0, random_state))\n",
    "], tablefmt='unsafehtml', stralign='right'))\n",
    "\n",
    "glue('rmpx_mpx_mapping_table', t, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modification to the traditional Boolean Multiplexer (MPX) was introduced by Wilson {cite}`wilson1999get` to examine the performance in single-step environments using real-valued data.\n",
    "\n",
    "````{margin}\n",
    "```{admonition} Epistasis and heterogeneity\n",
    "**Epistasis** describes an non-linear interaction effect between multiple features.\n",
    "\n",
    "**Heterogenity** means that for different sets of instances, a distinct subset of features will determine the class value.\n",
    "```\n",
    "````\n",
    "\n",
    "The Boolean _n_-bit multiplexer defines a set of single-step supervised learning problems that are conceptually based on an electronic device taking multiple inputs and switching them to the single output. In each trial a random, fixed binary string of predefined length is generated. It comprises two parts - the address and register segments. The first one points to specific register address that is considered to be a truth value - Figure {numref}`{number} <mpx6bit>` describes the process. This environment is also particularly interesting because it possesses the properties of _epistasis_  and _heterogeneity_, which are often present in within real-world problems such as bioinformatics, finance or behaviour modelling.\n",
    "\n",
    ":::{figure-md} mpx6bit\n",
    "<img src=\"../../_static/mpx6bit.png\" width=\"75%\">\n",
    "\n",
    "Visualization of determining the output value of 6 bit multiplexer. The address bit points to the first value of the register array that is considered the true output. Diagram taken from {cite}`urbanowicz2017introduction`.\n",
    ":::\n",
    "\n",
    "For the rMPX the only difference between boolean multiplexer is that generated perception consists of real-values drawn from a uniform distribution. To validate the correct answer, the additional variable - secret threshold $\\theta = 0.5$ is used to map each allele into binary form. See Table {numref}`{number} <rmpx_mpx_mapping_table>` for the example of such mapping.\n",
    "\n",
    "```{glue:figure} rmpx_mpx_mapping_table\n",
    ":align: center\n",
    ":figwidth: 500px\n",
    ":name: \"rmpx_mpx_mapping_table\"\n",
    "\n",
    "Example of mapping a random 6-bit rMPX signal into MPX problem. A threshold of $\\theta=0.5$ was used. The last bit is set to $0$ as an initial value that will be changed to $1$ after performing the correct action.\n",
    "```\n",
    "\n",
    "The standard version, however, is still not suitable to be used with ALCS. Because an agent utilizes perceptual causality to form new classifiers, assuming that after executing an action, the state will change. The MPX does not have any possibility to send feedback about the correctness of the action. Butz suggested two solutions to this problem {cite}`butz2002anticipatory`. In this work, the assumption is that the state generated by the rMPX is extended by one extra bit, denoting whether the classification was successful. This bit is by default set to zero the agent responds correctly after being switched, thus providing a direct feedback. A detailed example can be found in {cite}`kozlowski2019preliminary`.\n",
    "\n",
    "**Reward scheme**: If the correct answer is given the reward $r=1000$ is paid out, otherwise $r=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(section-topics-environments-checkerboard)=\n",
    "## Checkerboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pylab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgym_checkerboard\u001B[39;00m  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mpylab\u001B[49m\u001B[38;5;241m.\u001B[39mioff()\n\u001B[1;32m      5\u001B[0m checkerboard_env \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcheckerboard-2D-3div-v0\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m checkerboard_env\u001B[38;5;241m.\u001B[39mreset()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pylab' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_checkerboard(plot_filename=None):\n",
    "    import gym_checkerboard  # noqa: F401\n",
    "    checkerboard_env = gym.make('checkerboard-2D-3div-v0')\n",
    "    checkerboard_env.reset()\n",
    "\n",
    "    np_board = checkerboard_env.env._board.board\n",
    "\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.matshow(np_board, cmap=plt.get_cmap('gray_r'), extent=(0, 1, 0, 1), alpha=.5)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "    if plot_filename:\n",
    "        fig.savefig(plot_filename, dpi=PLOT_DPI)\n",
    "\n",
    "    return fig\n",
    "\n",
    "glue('checkerboard-env', plot_checkerboard(f'{plot_dir}/checkerboard-env-visualization.png'), display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Checkerboard is a single-step environment introduced by Stone in {cite}`stone2003real`. It was proposed to circumvent certain limitations of the [](section-topics-environments-rmpx) when using the interval predicates approach. Because the rMPX problem can be solved by using just a hyperplane decision surface, it's not considered of being able to represent arbitrary intervals in solution space. In order to solve the Checkerboard problem a hyper-rectangle decision surface is needed for modelling certain interval regions.\n",
    "\n",
    "This environment works by dividing up the $n$-dimensional solution space into equal-sized hypercubes. Each hypercube is assigned a white or black color (alternating in all dimensions), see Figure {numref}`{number} <checkerboard-env>`. The problem difficulty can be controlled by changing both the dimensionality $n$ and the number of divisions in each dimension $n_d$. In order to allow the colors to be alternating $n_d$ must be an odd number.\n",
    "\n",
    "```{glue:figure} checkerboard-env\n",
    ":name: \"checkerboard-env\"\n",
    "2-dimensional Checkerboard problem with $n_d=3$. Precise interval predicates are needed for representing selected regions.\n",
    "```\n",
    "In each trial the environment presents a vector of length $n_d$ or real numbers in the interval $[0, 1)$, representing a point in the solution space. The agent's goal is to guess the correct color by performing one of two actions depending on the colour (black or white) of a pointed hypercube.\n",
    "\n",
    "**Reward scheme**: When the correct answer is given the reward $r=1$ is paid out, otherwise $r=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(section-topics-environments-cartpole)=\n",
    "## Cart Pole\n",
    "\n",
    ":::{figure-md} cartpole-env\n",
    "<img src=\"../../_static/cartpole.gif\">\n",
    "\n",
    "The cartpole environment\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(section-topics-environments-fsw)=\n",
    "## Finite State Worlds (FSW)\n",
    "...\n",
    "\n",
    ":::{figure-md} fsw-env\n",
    "<img src=\"../../_static/fsw.png\">\n",
    "\n",
    "A Finite State World of length 5 (FSW-5). Environment is especially suited for measuring a reward propagation for long action chains.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(section-topics-environments-woods1)=\n",
    "## Woods1\n",
    "Why this environment was considered in this work?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}