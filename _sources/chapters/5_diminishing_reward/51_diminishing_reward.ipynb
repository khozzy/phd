{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Diminishing reward\n",
    "Why important for the real-valued settings. Introduce the AACS2 variant (for better dealing with situations where rewards are far away).\n",
    "Describe other approach like classifier bridging (page 13 in urbanowicz introduction, review, roadmap).\n",
    "\n",
    "## Experimental evaluation\n",
    "This section presents the motivation, goals and set-up of the performed experiments, as well as their results.\n",
    "\n",
    "## Research questions\n",
    "The conducted research aims to answers the following questions:\n",
    "\n",
    "1. Can the undiscounted reward criterion be used in discretized multistep environments?\n",
    "2. Does the undiscounted reward criterion result in better environment exploitation?\n",
    "\n",
    "## Goals of the experiments\n",
    "In all the experiments different environments are evaluated by two versions of AACS2 algorithm. To accent the changes benchmarking algorithms such as pure ACS2, Q-Learning and R-Learning are also used. The main performance metrics, such as number of steps to reach reward state and the estimated average reward are collected. Additionally, each available state-action pair from the environment is analyzed for the potential calculated reward.\n",
    "\n",
    "```{admonition} _Experiment 1 - Corridor environment_\n",
    "The [](section-topics-environments-corridor) environment provides a simple, real-valued and scalable benchmark perfectly suited for the problem of long action chains. The agent is required to repeat the same action certain amount of times unless reaching final reward state.\n",
    "```\n",
    "\n",
    "```{admonition} _Experiment 2 - Finite-State-World (FSW) environment_\n",
    "The [](section-topics-environments-fsw) environment is an extension to the Corridor. In each state actions alternate - one will bring the agent closer to the reward, the other one is languishing. The forager will have to learn to distinguish every other state and distribute rewards properly.\n",
    "```\n",
    "\n",
    "```{admonition} _Experiment 3 - Woods1 environment_\n",
    "Finally, the [](section-topics-environments-woods1) grid realm measures the agents performance in the case where a certain pattern containing the reward signal is repeated indefinitely in the horizonal and vertical directions.\n",
    "```\n",
    "\n",
    "## Experiments\n",
    "\n",
    "```{tableofcontents}\n",
    "```\n",
    "\n",
    "## Answers to research questions\n",
    "..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}