{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import warnings\n",
    "from typing import List, Dict\n",
    "\n",
    "import gym\n",
    "import gym_fsw  # noqa: F401\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lcs import Perception\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "from myst_nb import glue\n",
    "from IPython.display import HTML\n",
    "from tabulate import tabulate\n",
    "from src.bayes_estimation import bayes_estimate\n",
    "\n",
    "from src.basic_rl import run_q_learning_alternating, run_r_learning_alternating, qlearning, rlearning\n",
    "from src.commons import NUM_EXPERIMENTS\n",
    "from src.decorators import repeat, get_from_cache_or_run\n",
    "from src.diminishing_reward import common_metrics\n",
    "from src.observation_wrappers import FSWObservationWrapper\n",
    "from src.payoff_landscape import get_all_state_action, plot_payoff_landscape\n",
    "from src.runner import run_experiments_alternating\n",
    "from src.utils import build_plots_dir_path, build_cache_dir_path\n",
    "from src.visualization import PLOT_DPI, diminishing_reward_colors\n",
    "\n",
    "plt.ioff()  # turn off interactive plotting\n",
    "plt.style.use('../../../src/phd.mplstyle')\n",
    "\n",
    "root_dir = pathlib.Path().cwd().parent.parent.parent\n",
    "cwd_dir = pathlib.Path().cwd()\n",
    "\n",
    "plot_dir = build_plots_dir_path(root_dir) / cwd_dir.name\n",
    "cache_dir = build_cache_dir_path(root_dir) / cwd_dir.name\n",
    "\n",
    "def extract_specific_index(runs, env_idx):\n",
    "    \"\"\"Selects run metrics for certain environment, ie FSW 40\"\"\"\n",
    "    return [run[env_idx] for run in runs]\n",
    "\n",
    "def average_experiment_runs(run_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return run_df.groupby(['agent', 'trial', 'phase']).mean().reset_index(level='phase')\n",
    "\n",
    "def plot_pop_and_rho(df, trials, plot_filename=None):\n",
    "    colors = diminishing_reward_colors()\n",
    "\n",
    "    expl_df = df[df['phase'] == 'exploit']\n",
    "\n",
    "    xmax = trials/2\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(18, 16))\n",
    "\n",
    "    # Steps in trial plot\n",
    "    for alg in ['ACS2', 'AACS2_v1', 'AACS2_v2', 'Q-Learning', 'R-Learning']:\n",
    "        alg_df = expl_df.loc[alg]\n",
    "        idx = pd.Index(name='exploit trial', data=np.arange(1, len(alg_df) + 1))\n",
    "        alg_df.set_index(idx, inplace=True)\n",
    "\n",
    "        alg_df['steps_in_trial'].rolling(window=10).mean().plot(ax=axs[0], label=alg, linewidth=2, color=colors[alg])\n",
    "\n",
    "    axs[0].set_xlabel(\"Exploit trial\")\n",
    "    axs[0].set_xlim(1, 500)\n",
    "    axs[0].xaxis.set_major_locator(MultipleLocator(50))\n",
    "    axs[0].xaxis.set_minor_locator(MultipleLocator(10))\n",
    "    axs[0].xaxis.set_major_formatter(FormatStrFormatter('%1.0f'))\n",
    "    axs[0].xaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
    "    axs[0].xaxis.set_tick_params(which='minor', size=5, width=1, direction='in')\n",
    "\n",
    "    axs[0].set_ylabel(\"Number of steps\")\n",
    "    axs[0].yaxis.set_major_locator(MultipleLocator(1))\n",
    "    axs[0].yaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
    "    axs[0].yaxis.set_tick_params(which='minor', size=5, width=1, direction='in')\n",
    "\n",
    "    axs[0].set_title('Steps in trial')\n",
    "    axs[0].legend(loc='upper right', frameon=False)\n",
    "\n",
    "    # Rho plot\n",
    "    for alg in ['AACS2_v1', 'AACS2_v2', 'R-Learning']:\n",
    "        alg_df = expl_df.loc[alg]\n",
    "        idx = pd.Index(name='exploit trial', data=np.arange(1, len(alg_df) + 1))\n",
    "        alg_df.set_index(idx, inplace=True)\n",
    "\n",
    "        alg_df['rho'].plot(ax=axs[1], label=alg, linewidth=2, color=colors[alg])\n",
    "\n",
    "    axs[1].set_xlim(0, xmax)\n",
    "    axs[1].set_xlabel(\"Exploit trial\")\n",
    "    axs[1].xaxis.set_major_locator(MultipleLocator(500))\n",
    "    axs[1].xaxis.set_minor_locator(MultipleLocator(100))\n",
    "    axs[1].xaxis.set_major_formatter(FormatStrFormatter('%1.0f'))\n",
    "    axs[1].xaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
    "    axs[1].xaxis.set_tick_params(which='minor', size=5, width=1, direction='in')\n",
    "\n",
    "    axs[1].set_ylabel(r\"$\\mathregular{\\rho}$\")\n",
    "    axs[1].yaxis.set_major_locator(MultipleLocator(2))\n",
    "    axs[1].yaxis.set_minor_locator(MultipleLocator(1))\n",
    "    axs[1].yaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
    "    axs[1].yaxis.set_tick_params(which='minor', size=5, width=1, direction='in')\n",
    "    axs[1].set_ylim(0, 11)\n",
    "\n",
    "    axs[1].set_title(r'Estimated average $\\mathregular{\\rho}$')\n",
    "\n",
    "    if plot_filename:\n",
    "            fig.savefig(plot_filename, dpi=PLOT_DPI, bbox_inches='tight')\n",
    "\n",
    "    return fig\n",
    "\n",
    "# settings\n",
    "trials = 10_000\n",
    "USE_RAY= True\n",
    "\n",
    "learning_rate = 0.5\n",
    "discount_factor = 0.95\n",
    "epsilon = 0.1\n",
    "zeta = 0.0001\n",
    "\n",
    "glue('51-e2-trials', trials, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Experiment 2 - Deceptive Corridor\n",
    "[](section-topics-environments-fsw) differentiate from the Corridor by the presence of futile movement available in every step. Moreover, this action alternates between the states, demanding the agent to discriminate between relevant perceptions.\n",
    "\n",
    "Agent's behaviour is expected to properly distribute reward across all valuable states, meanwhile ignoring the rest of them. The analysis was performed by executing consecutive {glue:}`51-e2-trials` trials alternating between explore and exploit phases. Similarly, as in the previous experiment, there is only one state-observed; thus, the ACS2 genetic generalization mechanism remains turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def fsw10_env_provider():\n",
    "    import gym_fsw  # noqa: F401\n",
    "    return FSWObservationWrapper(gym.make(f'fsw-10-v0'))\n",
    "\n",
    "def fsw20_env_provider():\n",
    "    import gym_fsw  # noqa: F401\n",
    "    return FSWObservationWrapper(gym.make(f'fsw-20-v0'))\n",
    "\n",
    "def fsw40_env_provider():\n",
    "    import gym_fsw  # noqa: F401\n",
    "    return FSWObservationWrapper(gym.make(f'fsw-40-v0'))\n",
    "\n",
    "# Set ACS2/AACS2 configuration parameter dictionary\n",
    "basic_cfg = {\n",
    "    'perception_bits': 1,\n",
    "    'possible_actions': 2,\n",
    "    'do_ga': False,\n",
    "    'beta': learning_rate,\n",
    "    'epsilon': epsilon,\n",
    "    'gamma': discount_factor,\n",
    "    'zeta': zeta,\n",
    "    'user_metrics_collector_fcn': common_metrics,\n",
    "    'biased_exploration_prob': 0,\n",
    "    'metrics_trial_freq': 1\n",
    "}\n",
    "\n",
    "\n",
    "def run_multiple_qlearning(env_provider):\n",
    "    fsw_env = env_provider()\n",
    "    init_Q = np.zeros((fsw_env.observation_space.n, fsw_env.action_space.n))\n",
    "    return run_q_learning_alternating(NUM_EXPERIMENTS, trials, fsw_env, epsilon, learning_rate, discount_factor,\n",
    "                                      init_Q, perception_to_state_mapper=lambda p: int(p[0]))\n",
    "\n",
    "def run_multiple_rlearning(env_provider):\n",
    "    fsw_env = env_provider()\n",
    "    init_R = np.zeros((fsw_env.observation_space.n, fsw_env.action_space.n))\n",
    "    return run_r_learning_alternating(NUM_EXPERIMENTS, trials, fsw_env, epsilon, learning_rate, zeta, init_R,\n",
    "                                      perception_to_state_mapper=lambda p: int(p[0]))\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/fsw/acs2.dill')\n",
    "@repeat(num_times=NUM_EXPERIMENTS, use_ray=USE_RAY)\n",
    "def run_acs2():\n",
    "    fsw10 = run_experiments_alternating(fsw10_env_provider, trials, basic_cfg)\n",
    "    fsw20 = run_experiments_alternating(fsw20_env_provider, trials, basic_cfg)\n",
    "    fsw40 = run_experiments_alternating(fsw40_env_provider, trials, basic_cfg)\n",
    "    return fsw10, fsw20, fsw40\n",
    "\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/fsw/qlearning.dill')\n",
    "def run_qlearning():\n",
    "    fsw10 = run_multiple_qlearning(fsw10_env_provider)\n",
    "    fsw20 = run_multiple_qlearning(fsw20_env_provider)\n",
    "    fsw40 = run_multiple_qlearning(fsw40_env_provider)\n",
    "    return fsw10, fsw20, fsw40\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/fsw/rlearning.dill')\n",
    "def run_rlearning():\n",
    "    fsw10 = run_multiple_rlearning(fsw10_env_provider)\n",
    "    fsw20 = run_multiple_rlearning(fsw20_env_provider)\n",
    "    fsw40 = run_multiple_rlearning(fsw40_env_provider)\n",
    "    return fsw10, fsw20, fsw40\n",
    "\n",
    "# run computations\n",
    "acs2_runs_details = run_acs2()\n",
    "q_learning_runs = run_qlearning()\n",
    "r_learning_runs = run_rlearning()\n",
    "\n",
    "# average runs and create aggregated metrics data frame\n",
    "fsw10_acs2_metrics = pd.concat([m_df for _, _, _, m_df in extract_specific_index(acs2_runs_details, 0)])\n",
    "fsw10_qlearning_metrics = pd.DataFrame(q_learning_runs[0])\n",
    "fsw10_rlearning_metrics = pd.DataFrame(r_learning_runs[0])\n",
    "\n",
    "fsw20_acs2_metrics = pd.concat([m_df for _, _, _, m_df in extract_specific_index(acs2_runs_details, 1)])\n",
    "fsw20_qlearning_metrics = pd.DataFrame(q_learning_runs[1])\n",
    "fsw20_rlearning_metrics = pd.DataFrame(r_learning_runs[1])\n",
    "\n",
    "fsw40_acs2_metrics = pd.concat([m_df for _, _, _, m_df in extract_specific_index(acs2_runs_details, 2)])\n",
    "fsw40_qlearning_metrics = pd.DataFrame(q_learning_runs[2])\n",
    "fsw40_rlearning_metrics = pd.DataFrame(r_learning_runs[2])\n",
    "\n",
    "agg_df = pd.concat([\n",
    "    average_experiment_runs(fsw10_acs2_metrics),\n",
    "    average_experiment_runs(fsw10_qlearning_metrics),\n",
    "    average_experiment_runs(fsw10_rlearning_metrics)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_state_action_payoffs(state_actions: List, pop_acs2, pop_aacs2v1, pop_aacs2v2, Q, R) -> Dict:\n",
    "    payoffs = {}\n",
    "\n",
    "    for sa in state_actions:\n",
    "        p = Perception((sa.state,))\n",
    "\n",
    "        # ACS2\n",
    "        acs2_match_set = pop_acs2.form_match_set(p)\n",
    "        acs2_action_set = acs2_match_set.form_action_set(sa.action)\n",
    "\n",
    "        # AACS2_v1\n",
    "        aacs2v1_match_set = pop_aacs2v1.form_match_set(p)\n",
    "        aacs2v1_action_set = aacs2v1_match_set.form_action_set(sa.action)\n",
    "\n",
    "        # AACS2_v2\n",
    "        aacs2v2_match_set = pop_aacs2v2.form_match_set(p)\n",
    "        aacs2v2_action_set = aacs2v2_match_set.form_action_set(sa.action)\n",
    "\n",
    "        # Check if all states are covered\n",
    "        for alg, action_set in zip(['ACS2', 'AACS2_v1', 'AACS2_v2'],\n",
    "                                   [acs2_action_set, aacs2v1_action_set,\n",
    "                                    aacs2v2_action_set]):\n",
    "            if len(action_set) == 0:\n",
    "                warnings.warn(f\"No {alg} classifiers for perception: {p}, action: {sa.action}\")\n",
    "\n",
    "        payoffs[sa] = {\n",
    "            'ACS2': np.mean(list(map(lambda cl: cl.r, acs2_action_set))),\n",
    "            'AACS2_v1': np.mean(list(map(lambda cl: cl.r, aacs2v1_action_set))),\n",
    "            'AACS2_v2': np.mean(list(map(lambda cl: cl.r, aacs2v2_action_set))),\n",
    "            'Q-Learning': Q[int(sa.state), sa.action],\n",
    "            'R-Learning': R[int(sa.state), sa.action]\n",
    "        }\n",
    "\n",
    "    return payoffs\n",
    "\n",
    "\n",
    "# Take first of each algorithm population pass for presenting payoff landscape\n",
    "fsw_env = fsw10_env_provider()\n",
    "state_action = get_all_state_action(fsw_env.state_action())\n",
    "pop_acs2, pop_aacs2v1, pop_aacs2v2, _ = extract_specific_index(acs2_runs_details, 0)[0]\n",
    "\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/fsw/qlearning-single.dill')\n",
    "def run_single_qlearning():\n",
    "    Q_init = np.zeros((fsw_env.observation_space.n, fsw_env.action_space.n))\n",
    "    Q, _ = qlearning(fsw_env, trials, Q_init, epsilon, learning_rate, discount_factor, perception_to_state_mapper=lambda p: int(p[0]))\n",
    "    return Q\n",
    "\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/fsw/rlearning-single.dill')\n",
    "def run_single_rlearning():\n",
    "    R_init = np.zeros((fsw_env.observation_space.n, fsw_env.action_space.n))\n",
    "    R, rho, _ = rlearning(fsw_env, trials, R_init, epsilon, learning_rate, zeta, perception_to_state_mapper=lambda p: int(p[0]))\n",
    "    return R, rho\n",
    "\n",
    "\n",
    "Q = run_single_qlearning()\n",
    "R, rho = run_single_rlearning()\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    payoffs = calculate_state_action_payoffs(state_action, pop_acs2, pop_aacs2v1, pop_aacs2v2, Q, R)\n",
    "\n",
    "fsw_performance_fig = plot_pop_and_rho(agg_df, trials=trials, plot_filename=f'{plot_dir}/fsw-performance.png')\n",
    "fsw_payoff_fig = plot_payoff_landscape(payoffs, rho=rho, rho_text_location={'x': 30, 'y': 60}, plot_filename=f'{plot_dir}/fsw-payoff-landscape.png')\n",
    "\n",
    "glue('51-fsw-fig', fsw_performance_fig, display=False)\n",
    "glue('51-fsw-payoff-fig', fsw_payoff_fig, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "$\\beta=0.5$, $\\gamma=0.95$, $\\epsilon=0.1$, $\\theta_r = 0.9$, $\\theta_i=0.1$, $m_u=0$, $\\chi=0$, $\\zeta=0.0001$.\n",
    "\n",
    "````{tabbed} Performance\n",
    "```{glue:figure} 51-fsw-fig\n",
    ":name: \"51-fsw-fig\"\n",
    "Performance in FSW-10 environment. Plots are averaged over {glue:}`num_experiments` experiments. A moving average with window 25 was applied for the number of steps. Notice that the abscissa on both plots is scaled differently.\n",
    "```\n",
    "````\n",
    "\n",
    "````{tabbed} Payoff Landscape\n",
    "```{glue:figure} 51-fsw-payoff-fig\n",
    ":name: \"51-fsw-payoff-fig\"\n",
    "Payoff Landscape for the FSW-10 environment. Payoff values were obtained after {glue:}`51-e2-trials` trials. For the Q-Learning and R-Learning, the same learning parameters were applied.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Statistical verification\n",
    "To statistically assess the population size, the posterior data distribution was modelled using {glue:}`num_experiments` metric values collected in the last trial and then sampled with 100,000 draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "INFO:pymc3:Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "INFO:pymc3:Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "INFO:pymc3:Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [nu, std, mu]\n",
      "INFO:pymc3:NUTS: [nu, std, mu]\n",
      "Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 8 seconds.\n",
      "INFO:pymc3:Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 8 seconds.\n",
      "Auto-assigning NUTS sampler...\n",
      "INFO:pymc3:Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "INFO:pymc3:Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "INFO:pymc3:Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [nu, std, mu]\n",
      "INFO:pymc3:NUTS: [nu, std, mu]\n",
      "Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 7 seconds.\n",
      "INFO:pymc3:Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 7 seconds.\n",
      "Auto-assigning NUTS sampler...\n",
      "INFO:pymc3:Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "INFO:pymc3:Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "INFO:pymc3:Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [nu, std, mu]\n",
      "INFO:pymc3:NUTS: [nu, std, mu]\n",
      "Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 8 seconds.\n",
      "INFO:pymc3:Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 8 seconds.\n",
      "Auto-assigning NUTS sampler...\n",
      "INFO:pymc3:Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "INFO:pymc3:Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "INFO:pymc3:Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [nu, std, mu]\n",
      "INFO:pymc3:NUTS: [nu, std, mu]\n",
      "Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 7 seconds.\n",
      "INFO:pymc3:Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 7 seconds.\n",
      "Auto-assigning NUTS sampler...\n",
      "INFO:pymc3:Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "INFO:pymc3:Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "INFO:pymc3:Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [nu, std, mu]\n",
      "INFO:pymc3:NUTS: [nu, std, mu]\n",
      "Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 6 seconds.\n",
      "INFO:pymc3:Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 6 seconds.\n",
      "Auto-assigning NUTS sampler...\n",
      "INFO:pymc3:Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "INFO:pymc3:Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "INFO:pymc3:Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [nu, std, mu]\n",
      "INFO:pymc3:NUTS: [nu, std, mu]\n",
      "Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 8 seconds.\n",
      "INFO:pymc3:Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 8 seconds.\n",
      "Auto-assigning NUTS sampler...\n",
      "INFO:pymc3:Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "INFO:pymc3:Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "INFO:pymc3:Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [nu, std, mu]\n",
      "INFO:pymc3:NUTS: [nu, std, mu]\n",
      "Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 8 seconds.\n",
      "INFO:pymc3:Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 8 seconds.\n",
      "Auto-assigning NUTS sampler...\n",
      "INFO:pymc3:Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "INFO:pymc3:Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "INFO:pymc3:Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [nu, std, mu]\n",
      "INFO:pymc3:NUTS: [nu, std, mu]\n",
      "Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 6 seconds.\n",
      "INFO:pymc3:Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 6 seconds.\n",
      "Auto-assigning NUTS sampler...\n",
      "INFO:pymc3:Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "INFO:pymc3:Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "INFO:pymc3:Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [nu, std, mu]\n",
      "INFO:pymc3:NUTS: [nu, std, mu]\n",
      "Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 8 seconds.\n",
      "INFO:pymc3:Sampling 4 chains for 1_000 tune and 3_000 draw iterations (4_000 + 12_000 draws total) took 8 seconds.\n"
     ]
    },
    {
     "data": {
      "application/papermill.record/text/html": "<table>\n<thead>\n<tr><th style=\"text-align: center;\">                   </th><th style=\"text-align: center;\">   ACS2   </th><th style=\"text-align: center;\">  AACS2v1  </th><th style=\"text-align: center;\">  AACS2v2   </th><th style=\"text-align: center;\"> Q-Learning </th><th style=\"text-align: center;\"> R-Learning </th></tr>\n</thead>\n<tbody>\n<tr><td style=\"text-align: center;\">steps in last trial</td><td style=\"text-align: center;\">10.0 ± 0.0</td><td style=\"text-align: center;\">10.0 ± 0.0 </td><td style=\"text-align: center;\"> 10.0 ± 0.0 </td><td style=\"text-align: center;\"> 10.0 ± 0.0 </td><td style=\"text-align: center;\"> 10.0 ± 0.0 </td></tr>\n<tr><td style=\"text-align: center;\">  average reward   </td><td style=\"text-align: center;\">    -     </td><td style=\"text-align: center;\">9.93 ± 0.01</td><td style=\"text-align: center;\">10.01 ± 0.01</td><td style=\"text-align: center;\">     -      </td><td style=\"text-align: center;\"> 9.81 ± 0.0 </td></tr>\n</tbody>\n</table>",
      "application/papermill.record/text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "51-fsw10-bayes"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/html": "<table>\n<thead>\n<tr><th style=\"text-align: center;\">                   </th><th style=\"text-align: center;\">   ACS2   </th><th style=\"text-align: center;\">  AACS2v1  </th><th style=\"text-align: center;\"> AACS2v2  </th><th style=\"text-align: center;\"> Q-Learning </th><th style=\"text-align: center;\"> R-Learning </th></tr>\n</thead>\n<tbody>\n<tr><td style=\"text-align: center;\">steps in last trial</td><td style=\"text-align: center;\">20.0 ± 0.0</td><td style=\"text-align: center;\">20.0 ± 0.0 </td><td style=\"text-align: center;\">20.0 ± 0.0</td><td style=\"text-align: center;\"> 20.0 ± 0.0 </td><td style=\"text-align: center;\"> 20.0 ± 0.0 </td></tr>\n<tr><td style=\"text-align: center;\">  average reward   </td><td style=\"text-align: center;\">    -     </td><td style=\"text-align: center;\">4.99 ± 0.01</td><td style=\"text-align: center;\">5.0 ± 0.01</td><td style=\"text-align: center;\">     -      </td><td style=\"text-align: center;\"> 4.9 ± 0.0  </td></tr>\n</tbody>\n</table>",
      "application/papermill.record/text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "51-fsw20-bayes"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/html": "<table>\n<thead>\n<tr><th style=\"text-align: center;\">                   </th><th style=\"text-align: center;\">   ACS2   </th><th style=\"text-align: center;\"> AACS2v1  </th><th style=\"text-align: center;\">  AACS2v2  </th><th style=\"text-align: center;\"> Q-Learning </th><th style=\"text-align: center;\"> R-Learning </th></tr>\n</thead>\n<tbody>\n<tr><td style=\"text-align: center;\">steps in last trial</td><td style=\"text-align: center;\">40.0 ± 0.0</td><td style=\"text-align: center;\">40.0 ± 0.0</td><td style=\"text-align: center;\">40.0 ± 0.0 </td><td style=\"text-align: center;\"> 40.0 ± 0.0 </td><td style=\"text-align: center;\"> 40.0 ± 0.0 </td></tr>\n<tr><td style=\"text-align: center;\">  average reward   </td><td style=\"text-align: center;\">    -     </td><td style=\"text-align: center;\">2.51 ± 0.0</td><td style=\"text-align: center;\">2.51 ± 0.01</td><td style=\"text-align: center;\">     -      </td><td style=\"text-align: center;\"> 2.45 ± 0.0 </td></tr>\n</tbody>\n</table>",
      "application/papermill.record/text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "51-fsw40-bayes"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_models(dfs: Dict[str, pd.DataFrame], field: str, query_condition: str):\n",
    "    results = {}\n",
    "\n",
    "    for name, df in dfs.items():\n",
    "        data_arr = df.query(query_condition)[field].to_numpy()\n",
    "        bayes_model = bayes_estimate(data_arr)\n",
    "        results[name] = (bayes_model['mu'], bayes_model['std'])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "experiments_data = {\n",
    "    'fsw10_acs2': fsw10_acs2_metrics.query('agent == \"ACS2\"'),\n",
    "    'fsw10_aacs2v1': fsw10_acs2_metrics.query('agent == \"AACS2_v1\"'),\n",
    "    'fsw10_aacs2v2': fsw10_acs2_metrics.query('agent == \"AACS2_v2\"'),\n",
    "    'fsw10_qlearning': pd.DataFrame(q_learning_runs[0]),\n",
    "    'fsw10_rlearning': pd.DataFrame(r_learning_runs[0]),\n",
    "    \n",
    "    'fsw20_acs2': fsw20_acs2_metrics.query('agent == \"ACS2\"'),\n",
    "    'fsw20_aacs2v1': fsw20_acs2_metrics.query('agent == \"AACS2_v1\"'),\n",
    "    'fsw20_aacs2v2': fsw20_acs2_metrics.query('agent == \"AACS2_v2\"'),\n",
    "    'fsw20_qlearning': pd.DataFrame(q_learning_runs[1]),\n",
    "    'fsw20_rlearning': pd.DataFrame(r_learning_runs[1]),\n",
    "    \n",
    "    'fsw40_acs2': fsw40_acs2_metrics.query('agent == \"ACS2\"'),\n",
    "    'fsw40_aacs2v1': fsw40_acs2_metrics.query('agent == \"AACS2_v1\"'),\n",
    "    'fsw40_aacs2v2': fsw40_acs2_metrics.query('agent == \"AACS2_v2\"'),\n",
    "    'fsw40_qlearning': pd.DataFrame(q_learning_runs[2]),\n",
    "    'fsw40_rlearning': pd.DataFrame(r_learning_runs[2]),\n",
    "}\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/fsw/bayes/steps.dill')\n",
    "def build_steps_models(dfs: Dict[str, pd.DataFrame]):\n",
    "    return build_models(dfs, field='steps_in_trial', query_condition=f'trial == {trials - 1}')\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/fsw/bayes/rho.dill')\n",
    "def build_rho_models(dfs: Dict[str, pd.DataFrame]):\n",
    "    filtered_dfs = {}\n",
    "    for k, v in dfs.items():\n",
    "        if any(r_model for r_model in ['aacs2v1', 'aacs2v2', 'rlearning'] if k.endswith(r_model)):\n",
    "            filtered_dfs[k] = v\n",
    "\n",
    "    return build_models(filtered_dfs, field='rho', query_condition=f'trial == {trials - 1}')\n",
    "\n",
    "steps_models = build_steps_models(experiments_data)\n",
    "rho_models = build_rho_models(experiments_data)\n",
    "\n",
    "def print_bayes_table(name_prefix, steps_models, rho_models):\n",
    "    print_row = lambda r: f'{round(r[0].mean(), 2)} ± {round(r[0].std(), 2)}'\n",
    "    rho_data = [print_row(v) for name, v in rho_models.items() if name.startswith(name_prefix)]\n",
    "\n",
    "    bayes_table_data = [\n",
    "        ['steps in last trial'] + [print_row(v) for name, v in steps_models.items() if name.startswith(name_prefix)],\n",
    "        ['average reward per step', '-', rho_data[0], rho_data[1], '-', rho_data[2]]\n",
    "    ]\n",
    "\n",
    "    table = tabulate(bayes_table_data,\n",
    "                     headers=['', 'ACS2', 'AACS2v1', 'AACS2v2', 'Q-Learning', 'R-Learning'],\n",
    "                     tablefmt=\"html\", stralign='center')\n",
    "    return HTML(table)\n",
    "\n",
    "# add glue outputs\n",
    "glue('51-fsw10-bayes', print_bayes_table('fsw10', steps_models, rho_models), display=False)\n",
    "glue('51-fsw20-bayes', print_bayes_table('fsw20', steps_models, rho_models), display=False)\n",
    "glue('51-fsw40-bayes', print_bayes_table('fsw40', steps_models, rho_models), display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{tabbed} FSW 10\n",
    "{glue:}`51-fsw10-bayes`\n",
    "```\n",
    "\n",
    "```{tabbed} FSW 20\n",
    "{glue:}`51-fsw20-bayes`\n",
    "```\n",
    "\n",
    "```{tabbed} FSW 40\n",
    "{glue:}`51-fsw40-bayes`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Observations\n",
    "The size of the environment used on Figure {numref}`{number} <51-fsw-fig>` was chosen to be $n = 10$, resulting in $2n + 1 = 21$ distinct states. Statistical inference proves that the undiscounted versions of rewarding manage to properly discriminate between worthwhile states without fluctuations for all problem sizes.\n",
    "\n",
    "The same Figure depicts that agents can learn in a more challenging environment without problems. It takes about 250 trials to perform an optimal number of steps to reach the reward state. The $\\rho$ parameter converges with the same dynamics as the Corridor environment from the previous section.\n",
    "\n",
    "The payoff-landscape Figure {numref}`{number} <51-fsw-payoff-fig>` shows that the average value estimate is very close to the one calculated by the R-learning algorithm. The difference is primarily visible in the state-action pairs located afar from the final state. The discounted versions of the algorithms performed precisely the same."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}