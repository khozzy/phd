{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# put custom scripts to module path\n",
    "sys.path.append('../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, FuncFormatter, FormatStrFormatter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import gym\n",
    "import gym_woods  # noqa: F401\n",
    "\n",
    "from lcs import Perception\n",
    "from lcs.metrics import population_metrics\n",
    "\n",
    "from src.runner import run_experiments_alternating\n",
    "from src.observation_wrappers import WoodsBinaryAdapter\n",
    "from src.decorators import repeat, get_from_cache_or_run\n",
    "from src.basic_rl import run_q_learning_alternating, run_r_learning_alternating, qlearning, rlearning\n",
    "from src.visualization import diminishing_reward_colors, PLOT_DPI\n",
    "from src.payoff_landscape import plot_payoff_landscape\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "plt.ioff()  # turn off interactive plotting\n",
    "plt.style.use('../../../src/phd.mplstyle')\n",
    "\n",
    "cache_dir = f'{pathlib.Path().absolute()}/cache'\n",
    "plot_dir = f'{pathlib.Path().absolute()}/plots'\n",
    "\n",
    "\n",
    "def common_metrics(agent, env):\n",
    "    metrics = {}\n",
    "\n",
    "    pop = agent.get_population()\n",
    "    agent_name = agent.__class__.__name__\n",
    "\n",
    "    if hasattr(agent, 'rho'):\n",
    "        metrics['rho'] = agent.rho\n",
    "        agent_name += \"_v\" + agent.cfg.rho_update_version\n",
    "    else:\n",
    "        metrics['rho'] = 0\n",
    "\n",
    "    metrics['agent'] = agent_name\n",
    "    metrics['reliable'] = len([cl for cl in pop if cl.is_reliable()])\n",
    "\n",
    "    metrics.update(population_metrics(pop, env))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def average_experiment_runs(run_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return run_df.groupby(['agent', 'trial', 'phase']).mean().reset_index(level='phase')\n",
    "\n",
    "\n",
    "def woods_env_provider():\n",
    "    import gym_woods  # noqa: F401\n",
    "    return WoodsBinaryAdapter(gym.make(f'Woods1-v0'))\n",
    "\n",
    "def build_state_perception_map(woods_env):\n",
    "    result = {}\n",
    "    state_id = 0\n",
    "\n",
    "    for raw_state, actions in woods_env.unwrapped._state_action().items():  # actions possible in states\n",
    "        raw_perception = woods_env.unwrapped._perception(*(map(int, raw_state)))\n",
    "        result[state_id] = woods_env.observation(raw_perception)\n",
    "        state_id += 1\n",
    "\n",
    "    return result\n",
    "\n",
    "# build a representation of (state_id: perception of a state)\n",
    "env = woods_env_provider()\n",
    "state_perception = build_state_perception_map(env)\n",
    "\n",
    "def get_state_id(perception):\n",
    "    return list(state_perception.keys())[list(state_perception.values()).index(perception)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Experiment 3 - Woods1 environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.8\n",
    "discount_factor = 0.95\n",
    "epsilon = 0.8\n",
    "zeta = 0.0001\n",
    "\n",
    "\n",
    "# Set ACS2/AACS2 configuration parameter dictionary\n",
    "basic_cfg = {\n",
    "    'perception_bits': 24,\n",
    "    'possible_actions': 8,\n",
    "    'do_ga': True,\n",
    "    'beta': learning_rate,\n",
    "    'epsilon': epsilon,\n",
    "    'gamma': discount_factor,\n",
    "    'zeta': zeta,\n",
    "    'user_metrics_collector_fcn': common_metrics,\n",
    "    'biased_exploration_prob': 0,\n",
    "    'metrics_trial_freq': 1\n",
    "}\n",
    "\n",
    "NUM_EXPERIMENTS = 4\n",
    "trials = 50_000  # 50k\n",
    "\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/woods1/acs2.dill')\n",
    "@repeat(num_times=NUM_EXPERIMENTS, use_ray=True)\n",
    "def run_acs2_in_woods():\n",
    "    return run_experiments_alternating(woods_env_provider, trials, basic_cfg)\n",
    "\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/woods1/qlearning.dill')\n",
    "def run_qlearning_in_woods():\n",
    "    woods1_env = woods_env_provider()\n",
    "    init_Q = np.zeros((woods1_env.observation_space.n * 3, woods1_env.action_space.n))\n",
    "    return run_q_learning_alternating(NUM_EXPERIMENTS, trials, woods1_env, epsilon, learning_rate, discount_factor,\n",
    "                                      init_Q, perception_to_state_mapper=get_state_id)\n",
    "\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/woods1/rlearning.dill')\n",
    "def run_rlearning_in_woods():\n",
    "    woods1_env = woods_env_provider()\n",
    "    init_R = np.zeros((woods1_env.observation_space.n * 3, woods1_env.action_space.n))\n",
    "    return run_r_learning_alternating(NUM_EXPERIMENTS, trials, woods1_env, epsilon, learning_rate, zeta, init_R,\n",
    "                                      perception_to_state_mapper=get_state_id)\n",
    "\n",
    "\n",
    "# run computations\n",
    "acs2_runs_details = run_acs2_in_woods()\n",
    "q_learning_metrics = run_qlearning_in_woods()\n",
    "r_learning_metrics = run_rlearning_in_woods()\n",
    "\n",
    "# average runs and create aggregated metrics data frame\n",
    "acs2_metrics = [m_df for _, _, _, m_df in acs2_runs_details]\n",
    "\n",
    "agg_df = pd.concat([\n",
    "    average_experiment_runs(pd.concat(acs2_metrics)),\n",
    "    average_experiment_runs(pd.DataFrame(q_learning_metrics)),\n",
    "    average_experiment_runs(pd.DataFrame(r_learning_metrics))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_pop_and_rho(df, plot_filename=None):\n",
    "    colors = diminishing_reward_colors()\n",
    "\n",
    "    expl_df = df[df['phase'] == 'exploit']\n",
    "    xmax = trials/2\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(18, 16))\n",
    "\n",
    "    # Steps in trial plot\n",
    "    for alg in ['ACS2', 'AACS2_v1', 'AACS2_v2', 'Q-Learning', 'R-Learning']:\n",
    "        alg_df = expl_df.loc[alg]\n",
    "        idx = pd.Index(name='exploit trial', data=np.arange(1, len(alg_df) + 1))\n",
    "        alg_df.set_index(idx, inplace=True)\n",
    "\n",
    "        alg_df['steps_in_trial'].rolling(window=250).mean().plot(ax=axs[0], label=alg, linewidth=2, color=colors[alg])\n",
    "\n",
    "    axs[0].set_xlabel(\"Exploit trial\")\n",
    "    axs[0].set_xlim(0, xmax)\n",
    "    axs[0].xaxis.set_major_locator(MultipleLocator(5000))\n",
    "    axs[0].xaxis.set_minor_locator(MultipleLocator(1000))\n",
    "    axs[0].xaxis.set_major_formatter(FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "    axs[0].xaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
    "    axs[0].xaxis.set_tick_params(which='minor', size=5, width=1, direction='in')\n",
    "\n",
    "    axs[0].set_ylabel(\"Number of steps\")\n",
    "    axs[0].yaxis.set_major_locator(MultipleLocator(5))\n",
    "    axs[0].yaxis.set_minor_locator(MultipleLocator(1))\n",
    "    axs[0].yaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
    "    axs[0].yaxis.set_tick_params(which='minor', size=5, width=1, direction='in')\n",
    "\n",
    "    axs[0].set_title('Steps in trial')\n",
    "    axs[0].legend(loc='upper right', frameon=False)\n",
    "\n",
    "    # Rho plot\n",
    "    for alg in ['AACS2_v1', 'AACS2_v2', 'R-Learning']:\n",
    "        alg_df = expl_df.loc[alg]\n",
    "        idx = pd.Index(name='exploit trial', data=np.arange(1, len(alg_df) + 1))\n",
    "        alg_df.set_index(idx, inplace=True)\n",
    "\n",
    "        alg_df['rho'].rolling(window=1).mean().plot(ax=axs[1], label=alg, linewidth=2, color=colors[alg])\n",
    "\n",
    "    axs[1].set_xlabel(\"Exploit trial\")\n",
    "    axs[1].set_xlim(0, xmax)\n",
    "    axs[1].xaxis.set_major_locator(MultipleLocator(5000))\n",
    "    axs[1].xaxis.set_minor_locator(MultipleLocator(1000))\n",
    "    axs[1].xaxis.set_major_formatter(FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "    axs[1].xaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
    "    axs[1].xaxis.set_tick_params(which='minor', size=5, width=1, direction='in')\n",
    "\n",
    "    axs[1].set_ylabel(r\"$\\mathregular{\\rho}$\")\n",
    "    axs[1].yaxis.set_major_locator(MultipleLocator(100))\n",
    "    axs[1].yaxis.set_minor_locator(MultipleLocator(25))\n",
    "    axs[1].yaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
    "    axs[1].yaxis.set_tick_params(which='minor', size=5, width=1, direction='in')\n",
    "    axs[1].set_ylim(0, 500)\n",
    "\n",
    "    axs[1].set_title(r'Estimated average $\\mathregular{\\rho}$')\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if plot_filename:\n",
    "            fig.savefig(plot_filename, dpi=PLOT_DPI, bbox_inches='tight')\n",
    "\n",
    "plot_pop_and_rho(agg_df, plot_filename=f'{plot_dir}/woods1-performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ":::{figure-md} 51-woods1-fig\n",
    ":class: full-width\n",
    "<img src=\"plots/woods1-performance.png\">\n",
    "\n",
    "Performance in Woods1 environment\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Population plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_population(df, plot_filename=None):\n",
    "    colors = diminishing_reward_colors()\n",
    "\n",
    "    expl_df = df[df['phase'] == 'exploit']\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(18, 16))\n",
    "\n",
    "    x_lim = int(trials/10)\n",
    "\n",
    "    for idx, alg in enumerate(['ACS2', 'AACS2_v1', 'AACS2_v2']):\n",
    "        alg_df = expl_df.loc[alg]\n",
    "        index = pd.Index(name='exploit trial', data=np.arange(1, len(alg_df) + 1))\n",
    "        alg_df.set_index(index, inplace=True)\n",
    "\n",
    "        alg_df['reliable'].rolling(window=50).mean().plot(ax=axs[idx], label='reliable', linewidth=4, color=colors[alg])\n",
    "        alg_df['population'].rolling(window=50).mean().plot(ax=axs[idx], label='all', linewidth=1, color=colors[alg], alpha=0.8)\n",
    "\n",
    "        last_rel_count = alg_df['reliable'].iloc[x_lim]\n",
    "        last_pop_count = alg_df['population'].iloc[x_lim]\n",
    "        axs[idx].hlines(last_rel_count, 0, x_lim, linestyles='dashed', color=colors[alg], alpha=0.5)\n",
    "        axs[idx].hlines(last_pop_count, 0, x_lim, linestyles='dashed', color=colors[alg], alpha=0.5)\n",
    "        axs[idx].text(x_lim/20, last_rel_count-30, fr'${(last_pop_count-last_rel_count):.0f}$ non-reliable classifiers', color=colors[alg])\n",
    "\n",
    "        axs[idx].set_xlabel(\"Exploit trial\")\n",
    "        axs[idx].set_xlim(0, x_lim)\n",
    "        axs[idx].xaxis.set_major_locator(MultipleLocator(1000))\n",
    "        axs[idx].xaxis.set_minor_locator(MultipleLocator(500))\n",
    "        axs[idx].xaxis.set_major_formatter(FormatStrFormatter('%1.0f'))\n",
    "        axs[idx].xaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
    "        axs[idx].xaxis.set_tick_params(which='minor', size=5, width=1, direction='in')\n",
    "\n",
    "        axs[idx].set_ylabel(\"Number of classifiers\")\n",
    "        axs[idx].yaxis.set_major_locator(MultipleLocator(100))\n",
    "        axs[idx].yaxis.set_minor_locator(MultipleLocator(25))\n",
    "        axs[idx].yaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
    "        axs[idx].yaxis.set_tick_params(which='minor', size=5, width=1, direction='in')\n",
    "\n",
    "        axs[idx].set_title(f'{alg} population size')\n",
    "        axs[idx].legend(loc='upper right', frameon=False)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if plot_filename:\n",
    "        fig.savefig(plot_filename, dpi=PLOT_DPI, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plot_population(agg_df, plot_filename=f'{plot_dir}/woods1-population.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ":::{figure-md} 51-woods1-pop-fig\n",
    ":class: full-width\n",
    "<img src=\"plots/woods1-population.png\">\n",
    "\n",
    "Population of Woods1 environment\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Payoff landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: No AACS2_v2 classifiers for perception: 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0, action: 0\n",
      "WARN: No AACS2_v1 classifiers for perception: 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0, action: 2\n",
      "WARN: No AACS2_v1 classifiers for perception: 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0, action: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arc/miniconda3/envs/real-valued-acs-system/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/arc/miniconda3/envs/real-valued-acs-system/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "woods_env = woods_env_provider()\n",
    "\n",
    "def calculate_state_action_payoffs(state_actions: Dict, pop_acs2, pop_aacs2v1, pop_aacs2v2, Q, R) -> Dict:\n",
    "    payoffs = {}\n",
    "\n",
    "    for raw_state, actions in state_actions.items():  # actions possible in states\n",
    "        raw_perception = woods_env.unwrapped._perception(*(map(int, raw_state)))\n",
    "        p = Perception(woods_env.observation(raw_perception))\n",
    "        state_id = get_state_id(p)\n",
    "\n",
    "        for action in actions:\n",
    "            # ACS2\n",
    "            acs2_match_set = pop_acs2.form_match_set(p)\n",
    "            acs2_action_set = acs2_match_set.form_action_set(action)\n",
    "\n",
    "            # AACS2_v1\n",
    "            aacs2v1_match_set = pop_aacs2v1.form_match_set(p)\n",
    "            aacs2v1_action_set = aacs2v1_match_set.form_action_set(action)\n",
    "\n",
    "            # AACS2_v2\n",
    "            aacs2v2_match_set = pop_aacs2v2.form_match_set(p)\n",
    "            aacs2v2_action_set = aacs2v2_match_set.form_action_set(action)\n",
    "\n",
    "            # Check if all states are covered\n",
    "            for alg, action_set in zip(['ACS2', 'AACS2_v1', 'AACS2_v2'],\n",
    "                                       [acs2_action_set, aacs2v1_action_set,\n",
    "                                        aacs2v2_action_set]):\n",
    "                if len(action_set) == 0:\n",
    "                    print(f\"WARN: No {alg} classifiers for perception: {p}, action: {action}\")\n",
    "\n",
    "            payoffs[(state_id, action)] = {\n",
    "                'ACS2': np.mean(list(map(lambda cl: cl.r, acs2_action_set))),\n",
    "                'AACS2_v1': np.mean(list(map(lambda cl: cl.r, aacs2v1_action_set))),\n",
    "                'AACS2_v2': np.mean(list(map(lambda cl: cl.r, aacs2v2_action_set))),\n",
    "                'Q-Learning': Q[int(state_id), action],\n",
    "                'R-Learning': R[int(state_id), action]\n",
    "            }\n",
    "\n",
    "    return payoffs\n",
    "\n",
    "# Take first of each algorithm population pass for presenting payoff landscape\n",
    "pop_acs2, pop_aacs2v1, pop_aacs2v2, _ = acs2_runs_details[1]\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/woods1/qlearning-single.dill')\n",
    "def run_single_qlearning():\n",
    "    Q_init = np.zeros((woods_env.observation_space.n * 3, woods_env.action_space.n))\n",
    "    Q, _ = qlearning(woods_env, trials, Q_init, epsilon, learning_rate, discount_factor, perception_to_state_mapper=get_state_id)\n",
    "    return Q\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/woods1/rlearning-single.dill')\n",
    "def run_single_rlearning():\n",
    "    R_init = np.zeros((woods_env.observation_space.n * 3, woods_env.action_space.n))\n",
    "    R, rho, _ = rlearning(woods_env, trials, R_init, epsilon, learning_rate, zeta, perception_to_state_mapper=get_state_id)\n",
    "    return R, rho\n",
    "\n",
    "Q = run_single_qlearning()\n",
    "R, rho = run_single_rlearning()\n",
    "\n",
    "payoffs = calculate_state_action_payoffs(woods_env.unwrapped._state_action(), pop_acs2, pop_aacs2v1, pop_aacs2v2, Q, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plot_payoff_landscape(payoffs, rho=rho, rho_text_location={'x': 75, 'y': 450}, plot_filename=f'{plot_dir}/woods1-payoff-landscape.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ":::{figure-md} 51-woods1-payoff-landscape-fig\n",
    ":class: full-width\n",
    "<img src=\"plots/woods1-payoff-landscape.png\">\n",
    "\n",
    "Woods1 payoff landscape\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}