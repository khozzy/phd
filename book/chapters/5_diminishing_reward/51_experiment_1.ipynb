{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import warnings\n",
    "from typing import List, Dict\n",
    "\n",
    "import gym\n",
    "import gym_corridor  # noqa: F401\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "from lcs import Perception\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "from myst_nb import glue\n",
    "from tabulate import tabulate\n",
    "\n",
    "from src.basic_rl import run_q_learning_alternating, run_r_learning_alternating, qlearning, rlearning\n",
    "from src.bayes_estimation import bayes_estimate\n",
    "from src.commons import NUM_EXPERIMENTS\n",
    "from src.decorators import repeat, get_from_cache_or_run\n",
    "from src.diminishing_reward import common_metrics\n",
    "from src.observation_wrappers import CorridorObservationWrapper\n",
    "from src.payoff_landscape import get_all_state_action, plot_payoff_landscape\n",
    "from src.runner import run_experiments_alternating\n",
    "from src.utils import build_cache_dir_path, build_plots_dir_path\n",
    "from src.visualization import PLOT_DPI, diminishing_reward_colors\n",
    "\n",
    "plt.ioff()  # turn off interactive plotting\n",
    "plt.style.use('../../../src/phd.mplstyle')\n",
    "\n",
    "root_dir = pathlib.Path().cwd().parent.parent.parent\n",
    "cwd_dir = pathlib.Path().cwd()\n",
    "\n",
    "plot_dir = build_plots_dir_path(root_dir) / cwd_dir.name\n",
    "cache_dir = build_cache_dir_path(root_dir) / cwd_dir.name\n",
    "\n",
    "def extract_specific_index(runs, env_idx):\n",
    "    \"\"\"Selects run metrics for certain environment, ie Corridor 40\"\"\"\n",
    "    return [run[env_idx] for run in runs]\n",
    "\n",
    "def average_experiment_runs(run_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return run_df.groupby(['agent', 'trial', 'phase']).mean().reset_index(level='phase')\n",
    "\n",
    "def plot_pop_and_rho(df, trials, plot_filename=None):\n",
    "    colors = diminishing_reward_colors()\n",
    "\n",
    "    expl_df = df[df['phase'] == 'exploit']\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(18, 16), sharex=True)\n",
    "    xmax = trials / 2\n",
    "\n",
    "    # Steps in trial plot\n",
    "    for alg in ['ACS2', 'AACS2_v1', 'AACS2_v2', 'Q-Learning', 'R-Learning']:\n",
    "        alg_df = expl_df.loc[alg]\n",
    "        idx = pd.Index(name='exploit trial', data=np.arange(1, len(alg_df) + 1))\n",
    "        alg_df.set_index(idx, inplace=True)\n",
    "\n",
    "        alg_df['steps_in_trial'].rolling(window=250).mean().plot(ax=axs[0], label=alg, linewidth=2, color=colors[alg])\n",
    "\n",
    "    axs[0].set_xlim(0, xmax)\n",
    "    axs[0].set_xlabel(\"Exploit trial\")\n",
    "    axs[0].xaxis.set_major_locator(MultipleLocator(500))\n",
    "    axs[0].xaxis.set_minor_locator(MultipleLocator(100))\n",
    "    axs[0].xaxis.set_major_formatter(FormatStrFormatter('%1.0f'))\n",
    "    axs[0].xaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
    "    axs[0].xaxis.set_tick_params(which='minor', size=5, width=1, direction='in')\n",
    "\n",
    "    axs[0].set_ylabel(\"Number of steps\")\n",
    "    axs[0].set_yscale('log')\n",
    "    axs[0].set_title('Steps in trial')\n",
    "    axs[0].legend(loc='upper right', frameon=False)\n",
    "\n",
    "    # Rho plot\n",
    "    for alg in ['AACS2_v1', 'AACS2_v2', 'R-Learning']:\n",
    "        alg_df = expl_df.loc[alg]\n",
    "        idx = pd.Index(name='exploit trial', data=np.arange(1, len(alg_df) + 1))\n",
    "        alg_df.set_index(idx, inplace=True)\n",
    "\n",
    "        alg_df['rho'].rolling(window=1).mean().plot(ax=axs[1], label=alg, linewidth=2, color=colors[alg])\n",
    "\n",
    "    axs[1].set_xlim(0, xmax)\n",
    "    axs[1].set_xlabel(\"Exploit trial\")\n",
    "    axs[1].xaxis.set_major_locator(MultipleLocator(500))\n",
    "    axs[1].xaxis.set_minor_locator(MultipleLocator(100))\n",
    "    axs[1].xaxis.set_major_formatter(FormatStrFormatter('%1.0f'))\n",
    "    axs[1].xaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
    "    axs[1].xaxis.set_tick_params(which='minor', size=5, width=1, direction='in')\n",
    "\n",
    "    axs[1].set_ylabel(r\"$\\mathregular{\\rho}$\")\n",
    "    axs[1].yaxis.set_major_locator(MultipleLocator(25))\n",
    "    axs[1].yaxis.set_minor_locator(MultipleLocator(5))\n",
    "    axs[1].yaxis.set_tick_params(which='major', size=10, width=2, direction='in')\n",
    "    axs[1].yaxis.set_tick_params(which='minor', size=5, width=1, direction='in')\n",
    "    axs[1].set_ylim(0, 100)\n",
    "\n",
    "    axs[1].set_title(r'Estimated average $\\mathregular{\\rho}$')\n",
    "\n",
    "    if plot_filename:\n",
    "        fig.savefig(plot_filename, dpi=PLOT_DPI, bbox_inches='tight')\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Params\n",
    "trials = 10_000\n",
    "USE_RAY= True\n",
    "\n",
    "learning_rate = 0.8\n",
    "discount_factor = 0.95\n",
    "epsilon = 0.2\n",
    "zeta = 0.0001\n",
    "\n",
    "glue('51-e1-trials', trials, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Experiment 1 - Straight Corridor\n",
    "The following section describes the differences observed between using the ACS2 with standard discounted reward distribution and two proposed modifications. In all cases, the experiments were performed in an explore-exploit manner for the total number of {glue:}`51-e1-trials`, where the mode was alternating in each trial. Additionally, for better reference and benchmarking purposes, basic implementations of Q-Learning and R-Learning algorithms were also introduced and used with the same parameter settings as ACS2 and AACS2.\n",
    "\n",
    "The most important thing was to distinguish whether the new reward distribution proposition still allows the agent to successfully update the classifier's parameter to allow the exploitation of the environment. To illustrate this, figures presenting the number of steps to the final location, estimated average change during learning, and the reward payoff landscape across all possible state-action pairs were plotted for the [](section-topics-environments-corridor) of size $n=20$.\n",
    "\n",
    "To assure that the modification worked as expected, the statistical inference of obtained result was performed on a scaled version of the problem. Each experiment is averaged over {glue:}`num_experiments` independent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def corridor20_env_provider():\n",
    "    import gym_corridor  # noqa: F401\n",
    "    return CorridorObservationWrapper(gym.make(f'corridor-20-v0'))\n",
    "\n",
    "def corridor40_env_provider():\n",
    "    import gym_corridor  # noqa: F401\n",
    "    return CorridorObservationWrapper(gym.make(f'corridor-40-v0'))\n",
    "\n",
    "def corridor100_env_provider():\n",
    "    import gym_corridor  # noqa: F401\n",
    "    return CorridorObservationWrapper(gym.make(f'corridor-100-v0'))\n",
    "\n",
    "# Set ACS2/AACS2 configuration parameter dictionary\n",
    "basic_cfg = {\n",
    "    'perception_bits': 1,\n",
    "    'possible_actions': 2,\n",
    "    'do_ga': False,\n",
    "    'beta': learning_rate,\n",
    "    'epsilon': epsilon,\n",
    "    'gamma': discount_factor,\n",
    "    'zeta': zeta,\n",
    "    'user_metrics_collector_fcn': common_metrics,\n",
    "    'biased_exploration_prob': 0,\n",
    "    'metrics_trial_freq': 1\n",
    "}\n",
    "\n",
    "\n",
    "def run_multiple_qlearning(env_provider):\n",
    "    corridor_env = env_provider()\n",
    "    init_Q = np.zeros((corridor_env.observation_space.n, corridor_env.action_space.n))\n",
    "    return run_q_learning_alternating(NUM_EXPERIMENTS, trials, corridor_env, epsilon, learning_rate, discount_factor,\n",
    "                                      init_Q, perception_to_state_mapper=lambda p: int(p[0]))\n",
    "\n",
    "\n",
    "def run_multiple_rlearning(env_provider):\n",
    "    corridor_env = env_provider()\n",
    "    init_R = np.zeros((corridor_env.observation_space.n, corridor_env.action_space.n))\n",
    "    return run_r_learning_alternating(NUM_EXPERIMENTS, trials, corridor_env, epsilon, learning_rate, zeta, init_R,\n",
    "                                      perception_to_state_mapper=lambda p: int(p[0]))\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/corridor/acs2.dill')\n",
    "@repeat(num_times=NUM_EXPERIMENTS, use_ray=USE_RAY)\n",
    "def run_acs2():\n",
    "    corridor20 = run_experiments_alternating(corridor20_env_provider, trials, basic_cfg)\n",
    "    corridor40 = run_experiments_alternating(corridor40_env_provider, trials, basic_cfg)\n",
    "    corridor100 = run_experiments_alternating(corridor100_env_provider, trials, basic_cfg)\n",
    "    return corridor20, corridor40, corridor100\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/corridor/qlearning.dill')\n",
    "def run_qlearning():\n",
    "    corridor20 = run_multiple_qlearning(corridor20_env_provider)\n",
    "    corridor40 = run_multiple_qlearning(corridor40_env_provider)\n",
    "    corridor100 = run_multiple_qlearning(corridor100_env_provider)\n",
    "    return corridor20, corridor40, corridor100\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/corridor/rlearning.dill')\n",
    "def run_rlearning():\n",
    "    corridor20 = run_multiple_rlearning(corridor20_env_provider)\n",
    "    corridor40 = run_multiple_rlearning(corridor40_env_provider)\n",
    "    corridor100 = run_multiple_rlearning(corridor100_env_provider)\n",
    "    return corridor20, corridor40, corridor100\n",
    "\n",
    "# run computations\n",
    "acs2_runs_details = run_acs2()\n",
    "q_learning_runs = run_qlearning()\n",
    "r_learning_runs = run_rlearning()\n",
    "\n",
    "# average runs and create aggregated metrics data frame\n",
    "corridor20_acs2_metrics = pd.concat([m_df for _, _, _, m_df in extract_specific_index(acs2_runs_details, 0)])\n",
    "corridor20_qlearning_metrics = pd.DataFrame(q_learning_runs[0])\n",
    "corridor20_rlearning_metrics = pd.DataFrame(r_learning_runs[0])\n",
    "\n",
    "corridor40_acs2_metrics = pd.concat([m_df for _, _, _, m_df in extract_specific_index(acs2_runs_details, 1)])\n",
    "corridor40_qlearning_metrics = pd.DataFrame(q_learning_runs[1])\n",
    "corridor40_rlearning_metrics = pd.DataFrame(r_learning_runs[1])\n",
    "\n",
    "corridor100_acs2_metrics = pd.concat([m_df for _, _, _, m_df in extract_specific_index(acs2_runs_details, 2)])\n",
    "corridor100_qlearning_metrics = pd.DataFrame(q_learning_runs[2])\n",
    "corridor100_rlearning_metrics = pd.DataFrame(r_learning_runs[2])\n",
    "\n",
    "agg_df = pd.concat([\n",
    "    average_experiment_runs(corridor20_acs2_metrics),\n",
    "    average_experiment_runs(corridor20_qlearning_metrics),\n",
    "    average_experiment_runs(corridor20_rlearning_metrics)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# payoff landscape\n",
    "def calculate_state_action_payoffs(state_actions: List, pop_acs2, pop_aacs2v1, pop_aacs2v2, Q, R) -> Dict:\n",
    "    payoffs = {}\n",
    "\n",
    "    for sa in state_actions:\n",
    "        p = Perception((sa.state,))\n",
    "\n",
    "        # ACS2\n",
    "        acs2_match_set = pop_acs2.form_match_set(p)\n",
    "        acs2_action_set = acs2_match_set.form_action_set(sa.action)\n",
    "\n",
    "        # AACS2_v1\n",
    "        aacs2v1_match_set = pop_aacs2v1.form_match_set(p)\n",
    "        aacs2v1_action_set = aacs2v1_match_set.form_action_set(sa.action)\n",
    "\n",
    "        # AACS2_v2\n",
    "        aacs2v2_match_set = pop_aacs2v2.form_match_set(p)\n",
    "        aacs2v2_action_set = aacs2v2_match_set.form_action_set(sa.action)\n",
    "\n",
    "        # Check if all states are covered\n",
    "        for alg, action_set in zip(['ACS2', 'AACS2_v1', 'AACS2_v2'],\n",
    "                                   [acs2_action_set, aacs2v1_action_set,\n",
    "                                    aacs2v2_action_set]):\n",
    "            if len(action_set) == 0:\n",
    "                warnings.warn(f\"No {alg} classifiers for perception: {p}, action: {sa.action}\")\n",
    "\n",
    "        payoffs[sa] = {\n",
    "            'ACS2': np.mean(list(map(lambda cl: cl.r, acs2_action_set))),\n",
    "            'AACS2_v1': np.mean(list(map(lambda cl: cl.r, aacs2v1_action_set))),\n",
    "            'AACS2_v2': np.mean(list(map(lambda cl: cl.r, aacs2v2_action_set))),\n",
    "            'Q-Learning': Q[int(sa.state), sa.action],\n",
    "            'R-Learning': R[int(sa.state), sa.action]\n",
    "        }\n",
    "\n",
    "    return payoffs\n",
    "\n",
    "\n",
    "# Take first of each algorithm population pass for presenting payoff landscape\n",
    "corridor_env = corridor20_env_provider()\n",
    "state_action = get_all_state_action(corridor_env.unwrapped._state_action())\n",
    "pop_acs2, pop_aacs2v1, pop_aacs2v2, _ = extract_specific_index(acs2_runs_details, 0)[0]\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/corridor/qlearning-single.dill')\n",
    "def run_single_qlearning():\n",
    "    Q_init = np.zeros((corridor_env.observation_space.n, corridor_env.action_space.n))\n",
    "    Q, _ = qlearning(corridor_env, trials, Q_init, epsilon, learning_rate, discount_factor, perception_to_state_mapper=lambda p: int(p[0]))\n",
    "    return Q\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/corridor/rlearning-single.dill')\n",
    "def run_single_rlearning():\n",
    "    R_init = np.zeros((corridor_env.observation_space.n, corridor_env.action_space.n))\n",
    "    R, rho, _ = rlearning(corridor_env, trials, R_init, epsilon, learning_rate, zeta, perception_to_state_mapper=lambda p: int(p[0]))\n",
    "    return R, rho\n",
    "\n",
    "\n",
    "Q = run_single_qlearning()\n",
    "R, rho = run_single_rlearning()\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    payoffs = calculate_state_action_payoffs(state_action, pop_acs2, pop_aacs2v1, pop_aacs2v2, Q, R)\n",
    "\n",
    "corridor_performance_fig = plot_pop_and_rho(agg_df, trials=trials, plot_filename=f'{plot_dir}/corridor-performance.png')\n",
    "corridor_payoff_fig = plot_payoff_landscape(payoffs, rho=rho, rho_text_location={'x': 18, 'y': 250}, plot_filename=f'{plot_dir}/corridor-payoff-landscape.png')\n",
    "\n",
    "glue('51-corridor-fig', corridor_performance_fig, display=False)\n",
    "glue('51-corridor-payoff-fig',corridor_payoff_fig , display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "$\\beta=0.8$, $\\gamma=0.95$, $\\epsilon=0.2$, $\\theta_r = 0.9$, $\\theta_i=0.1$, $m_u=0$, $\\chi=0$, $\\zeta=0.0001$\n",
    "\n",
    "````{tabbed} Performance\n",
    "```{glue:figure} 51-corridor-fig\n",
    ":name: \"51-corridor-fig\"\n",
    "Performance in Corridor-20 environment. Plots averaged over {glue:}`num_experiments` independent runs. Number of steps in exploit trials is averaged over 250 last data points.\n",
    "```\n",
    "````\n",
    "\n",
    "````{tabbed} Payoff Landscape\n",
    "```{glue:figure} 51-corridor-payoff-fig\n",
    ":name: \"51-corridor-payoff-fig\"\n",
    "Payoff Landscape for Corridor  20 environment. Payoff values were obtained after {glue:}`51-e1-trials` trials. For the Q-Learning and R-Learning, the same learning parameters were applied. The ACS2 and Q-Learning generate exactly the same payoffs for each state-action pair.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Statistical verification\n",
    "To statistically assess the population size, the posterior data distribution was generated using the BEST method with {glue:}`num_experiments` metric values collected in the last trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/papermill.record/text/html": "<table>\n<thead>\n<tr><th style=\"text-align: center;\">                   </th><th style=\"text-align: center;\">   ACS2    </th><th style=\"text-align: center;\">  AACS2v1   </th><th style=\"text-align: center;\">  AACS2v2   </th><th style=\"text-align: center;\"> Q-Learning </th><th style=\"text-align: center;\"> R-Learning </th></tr>\n</thead>\n<tbody>\n<tr><td style=\"text-align: center;\">steps in last trial</td><td style=\"text-align: center;\">9.61 ± 0.84</td><td style=\"text-align: center;\">9.54 ± 0.84 </td><td style=\"text-align: center;\"> 9.0 ± 0.82 </td><td style=\"text-align: center;\">7.91 ± 0.79 </td><td style=\"text-align: center;\"> 9.31 ± 0.9 </td></tr>\n<tr><td style=\"text-align: center;\">  average reward   </td><td style=\"text-align: center;\">     -     </td><td style=\"text-align: center;\">88.52 ± 0.16</td><td style=\"text-align: center;\">88.65 ± 0.29</td><td style=\"text-align: center;\">     -      </td><td style=\"text-align: center;\">89.9 ± 0.19 </td></tr>\n</tbody>\n</table>",
      "application/papermill.record/text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "51-corridor20-bayes"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/html": "<table>\n<thead>\n<tr><th style=\"text-align: center;\">                   </th><th style=\"text-align: center;\">    ACS2    </th><th style=\"text-align: center;\">  AACS2v1  </th><th style=\"text-align: center;\">  AACS2v2   </th><th style=\"text-align: center;\">  Q-Learning  </th><th style=\"text-align: center;\"> R-Learning </th></tr>\n</thead>\n<tbody>\n<tr><td style=\"text-align: center;\">steps in last trial</td><td style=\"text-align: center;\">19.42 ± 1.65</td><td style=\"text-align: center;\">19.43 ± 1.7</td><td style=\"text-align: center;\">19.47 ± 1.46</td><td style=\"text-align: center;\">124.96 ± 14.21</td><td style=\"text-align: center;\">17.64 ± 1.67</td></tr>\n<tr><td style=\"text-align: center;\">  average reward   </td><td style=\"text-align: center;\">     -      </td><td style=\"text-align: center;\">44.7 ± 0.14</td><td style=\"text-align: center;\">44.7 ± 0.19 </td><td style=\"text-align: center;\">      -       </td><td style=\"text-align: center;\">44.93 ± 0.12</td></tr>\n</tbody>\n</table>",
      "application/papermill.record/text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "51-corridor40-bayes"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/papermill.record/text/html": "<table>\n<thead>\n<tr><th style=\"text-align: center;\">                   </th><th style=\"text-align: center;\">    ACS2    </th><th style=\"text-align: center;\">  AACS2v1   </th><th style=\"text-align: center;\">  AACS2v2   </th><th style=\"text-align: center;\"> Q-Learning </th><th style=\"text-align: center;\"> R-Learning </th></tr>\n</thead>\n<tbody>\n<tr><td style=\"text-align: center;\">steps in last trial</td><td style=\"text-align: center;\">55.95 ± 4.13</td><td style=\"text-align: center;\">51.25 ± 4.19</td><td style=\"text-align: center;\">49.83 ± 4.48</td><td style=\"text-align: center;\">200.0 ± 0.0 </td><td style=\"text-align: center;\">48.65 ± 4.37</td></tr>\n<tr><td style=\"text-align: center;\">  average reward   </td><td style=\"text-align: center;\">     -      </td><td style=\"text-align: center;\">17.84 ± 0.09</td><td style=\"text-align: center;\">17.75 ± 0.13</td><td style=\"text-align: center;\">     -      </td><td style=\"text-align: center;\">17.85 ± 0.07</td></tr>\n</tbody>\n</table>",
      "application/papermill.record/text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "51-corridor100-bayes"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_models(dfs: Dict[str, pd.DataFrame], field: str, query_condition: str):\n",
    "    results = {}\n",
    "\n",
    "    for name, df in dfs.items():\n",
    "        data_arr = df.query(query_condition)[field].to_numpy()\n",
    "        bayes_model = bayes_estimate(data_arr)\n",
    "        results[name] = (bayes_model['mu'], bayes_model['std'])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "experiments_data = {\n",
    "    'corridor20_acs2': corridor20_acs2_metrics.query('agent == \"ACS2\"'),\n",
    "    'corridor20_aacs2v1': corridor20_acs2_metrics.query('agent == \"AACS2_v1\"'),\n",
    "    'corridor20_aacs2v2': corridor20_acs2_metrics.query('agent == \"AACS2_v2\"'),\n",
    "    'corridor20_qlearning': pd.DataFrame(q_learning_runs[0]),\n",
    "    'corridor20_rlearning': pd.DataFrame(r_learning_runs[0]),\n",
    "    \n",
    "    'corridor40_acs2': corridor40_acs2_metrics.query('agent == \"ACS2\"'),\n",
    "    'corridor40_aacs2v1': corridor40_acs2_metrics.query('agent == \"AACS2_v1\"'),\n",
    "    'corridor40_aacs2v2': corridor40_acs2_metrics.query('agent == \"AACS2_v2\"'),\n",
    "    'corridor40_qlearning': pd.DataFrame(q_learning_runs[1]),\n",
    "    'corridor40_rlearning': pd.DataFrame(r_learning_runs[1]),\n",
    "    \n",
    "    'corridor100_acs2': corridor100_acs2_metrics.query('agent == \"ACS2\"'),\n",
    "    'corridor100_aacs2v1': corridor100_acs2_metrics.query('agent == \"AACS2_v1\"'),\n",
    "    'corridor100_aacs2v2': corridor100_acs2_metrics.query('agent == \"AACS2_v2\"'),\n",
    "    'corridor100_qlearning': pd.DataFrame(q_learning_runs[2]),\n",
    "    'corridor100_rlearning': pd.DataFrame(r_learning_runs[2]),\n",
    "}\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/corridor/bayes/steps.dill')\n",
    "def build_steps_models(dfs: Dict[str, pd.DataFrame]):\n",
    "    return build_models(dfs, field='steps_in_trial', query_condition=f'trial == {trials - 1}')\n",
    "\n",
    "@get_from_cache_or_run(cache_path=f'{cache_dir}/corridor/bayes/rho.dill')\n",
    "def build_rho_models(dfs: Dict[str, pd.DataFrame]):\n",
    "    filtered_dfs = {}\n",
    "    for k, v in dfs.items():\n",
    "        if any(r_model for r_model in ['aacs2v1', 'aacs2v2', 'rlearning'] if k.endswith(r_model)):\n",
    "            filtered_dfs[k] = v\n",
    "\n",
    "    return build_models(filtered_dfs, field='rho', query_condition=f'trial == {trials - 1}')\n",
    "\n",
    "steps_models = build_steps_models(experiments_data)\n",
    "rho_models = build_rho_models(experiments_data)\n",
    "\n",
    "def print_bayes_table(name_prefix, steps_models, rho_models):\n",
    "    print_row = lambda r: f'{round(r[0].mean(), 2)} ± {round(r[0].std(), 2)}'\n",
    "    rho_data = [print_row(v) for name, v in rho_models.items() if name.startswith(name_prefix)]\n",
    "\n",
    "    bayes_table_data = [\n",
    "        ['steps in last trial'] + [print_row(v) for name, v in steps_models.items() if name.startswith(name_prefix)],\n",
    "        ['average reward per step', '-', rho_data[0], rho_data[1], '-', rho_data[2]]\n",
    "    ]\n",
    "\n",
    "    table = tabulate(bayes_table_data,\n",
    "                     headers=['', 'ACS2', 'AACS2v1', 'AACS2v2', 'Q-Learning', 'R-Learning'],\n",
    "                     tablefmt=\"html\", stralign='center')\n",
    "    return HTML(table)\n",
    "\n",
    "# add glue outputs\n",
    "glue('51-corridor20-bayes', print_bayes_table('corridor20', steps_models, rho_models), display=False)\n",
    "glue('51-corridor40-bayes', print_bayes_table('corridor40', steps_models, rho_models), display=False)\n",
    "glue('51-corridor100-bayes', print_bayes_table('corridor100', steps_models, rho_models), display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```{tabbed} Corridor 20\n",
    "{glue:}`51-corridor20-bayes`\n",
    "```\n",
    "\n",
    "```{tabbed} Corridor 40\n",
    "{glue:}`51-corridor40-bayes`\n",
    "```\n",
    "\n",
    "```{tabbed} Corridor 100\n",
    "{glue:}`51-corridor100-bayes`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Observations\n",
    "The average number of steps can be calculated $\\frac{\\sum_0^{n} n}{n-1}$, where $n$ is the number of distinct Corridor states. For the tested environment it gives the approximate value of $11.05$, therefore the average reward per step estimation should be close to $1000 / 11.05 = 90.49$, which corresponds to the Figure {numref}`{number} <51-corridor-fig>`.\n",
    "\n",
    "The same Figure demonstrates that all investigated agents learned the environments. The anticipatory classifier systems obtained an optimal number of steps after the same number of exploit trials, which is about 200. In addition, the AACS2-v2 updates the $\\rho$ value more aggressively in earlier phases, but the estimate converges near the optimal reward per step.\n",
    "\n",
    "For the payoff-landscape in Figure {numref}`{number} <51-corridor-payoff-fig>`, all allowed state--action pairs were identified in the environment (38 in this case). The final population of learning classifiers was established after 100 trials and was the same size. Both Q-table and R-learning tables were populated using the same parameters and number of trials.\n",
    "\n",
    "The relative distance between adjacent state-action pairs can be divided into three groups. The first one relates to the discounted reward agents (ACS2, Q-Learning). Both generate almost a similar reward payoff for each state--action. Later, there is the R-Learning algorithm, which estimates the $\\rho$ value and separates states evenly. Furthermore, two AACS2 agents are performing very similarly. The $\\rho$ value calculated by the R-Learning algorithm is lower than the average estimation by the AACS2 algorithm.\n",
    "\n",
    "Scaled problem instances revealed interesting properties:\n",
    "- the Q-Learning algorithm was not capable of executing the optimal number of steps in environments with $n=40$, $n=100$,\n",
    "- for the most complex problem of $n=1000$, the AACS2 modification yield better performance than ACS2,\n",
    "- all algorithms with undiscounted reward critera managed to calculate the average reward $\\rho$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}