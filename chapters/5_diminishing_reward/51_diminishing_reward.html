
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>5.1. Diminishing reward &#8212; Real-valued Anticipatory Classifier System</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Experiment 1 - Straight Corridor" href="51_experiment_1.html" />
    <link rel="prev" title="5. Optimizing distributing reward through long action chains" href="51_introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/pwr_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Real-valued Anticipatory Classifier System</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Real-valued Anticipatory Classifier System
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../1_introduction/11_introduction.html">
   1. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_introduction/12_motivation.html">
     1.1. Motivation and challenges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_introduction/13_aims_goals.html">
     1.2. Research hypothesis, its aims and goals
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../1_introduction/14_structure.html">
     1.3. Thesis structure
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../2_selected_topics/21_introduction.html">
   2. Selected topics of Learning Classifier Systems
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_selected_topics/22_alcs_history.html">
     2.1. Road towards Anticipatory Learning Classifier Systems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_selected_topics/23_real_value_challenge.html">
     2.2. Real-valued signal challenge
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_selected_topics/24_kpi.html">
     2.3. Key performance indicators
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_selected_topics/25_stats.html">
     2.4. Statistical verification of results
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2_selected_topics/26_envs.html">
     2.5. Overview of the selected environments
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../3_internalizing/31_introduction.html">
   3. Ways of handling real-valued input signal
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../3_internalizing/interval/32_interval_based_representation.html">
     3.1. Interval-based representation
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
    <label for="toctree-checkbox-4">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../3_internalizing/interval/32_experiment_1.html">
       Experiment 1 - Encoding precision
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../3_internalizing/interval/32_experiment_2.html">
       Experiment 2 - Nature of the intervals
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../3_internalizing/discretization/33_discretizing_input_signal.html">
     3.2. Discretizing input signal
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../3_internalizing/discretization/33_experiment_3.html">
       Experiment 3 - Single-step environment performance
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../3_internalizing/discretization/33_experiment_4.html">
       Experiment 4 - Multiple-step environments performance
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../4_biased_exploration/41_introduction.html">
   4. Optimizing formation of internal model
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../4_biased_exploration/41_biased_exploration.html">
     4.1. Biased exploration
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../4_biased_exploration/41_experiment_1.html">
       Experiment 1 - Single-step problem performance
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../4_biased_exploration/41_experiment_2.html">
       Experiment 2 - Multi-steps problems performance
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../4_biased_exploration/41_experiment_3.html">
       Experiment 3 - Balancing the pole
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="51_introduction.html">
   5. Optimizing distributing reward through long action chains
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active has-children">
    <a class="current reference internal" href="#">
     5.1. Diminishing reward
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
    <label for="toctree-checkbox-9">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="51_experiment_1.html">
       Experiment 1 - Straight Corridor
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="51_experiment_2.html">
       Experiment 2 - Deceptive Corridor
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../6_summary/61_summary.html">
   6. Summary
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_summary/62_conclusions.html">
     6.1. Conclusions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_summary/63_future.html">
     6.2. Future Works
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../6_summary/64_publications.html">
     6.3. Publications
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../abbreviations.html">
   7. Abbreviations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../bibliography.html">
   8. Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/chapters/5_diminishing_reward/51_diminishing_reward.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/khozzy/phd"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/khozzy/phd/issues/new?title=Issue%20on%20page%20%2Fchapters/5_diminishing_reward/51_diminishing_reward.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/khozzy/phd/gh-pages?urlpath=tree/chapters/5_diminishing_reward/51_diminishing_reward.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reinforcement-learning-and-reward-criterion">
   Reinforcement Learning and Reward Criterion
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discounted-reward-criterion">
     Discounted Reward Criterion
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#undiscounted-averaged-reward-criterion">
     Undiscounted (Averaged) Reward Criterion
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#integrating-reward-criterions-in-acs2">
   Integrating Reward Criterions in ACS2
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aacs2-v1">
     AACS2-v1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aacs2-v2">
     AACS2-v2
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experimental-evaluation">
   Experimental evaluation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#research-questions">
     Research questions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#goals-of-the-experiments">
     Goals of the experiments
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#experiments">
     Experiments
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#answers-to-research-questions">
     Answers to research questions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#can-the-undiscounted-reward-criterion-be-used-in-discretized-multistep-environments">
       Can the undiscounted reward criterion be used in discretized multistep environments?
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#does-the-undiscounted-reward-criterion-result-in-better-environment-exploitation">
       Does the undiscounted reward criterion result in better environment exploitation?
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Diminishing reward</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reinforcement-learning-and-reward-criterion">
   Reinforcement Learning and Reward Criterion
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discounted-reward-criterion">
     Discounted Reward Criterion
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#undiscounted-averaged-reward-criterion">
     Undiscounted (Averaged) Reward Criterion
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#integrating-reward-criterions-in-acs2">
   Integrating Reward Criterions in ACS2
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aacs2-v1">
     AACS2-v1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aacs2-v2">
     AACS2-v2
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experimental-evaluation">
   Experimental evaluation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#research-questions">
     Research questions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#goals-of-the-experiments">
     Goals of the experiments
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#experiments">
     Experiments
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#answers-to-research-questions">
     Answers to research questions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#can-the-undiscounted-reward-criterion-be-used-in-discretized-multistep-environments">
       Can the undiscounted reward criterion be used in discretized multistep environments?
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#does-the-undiscounted-reward-criterion-result-in-better-environment-exploitation">
       Does the undiscounted reward criterion result in better environment exploitation?
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="diminishing-reward">
<h1><span class="section-number">5.1. </span>Diminishing reward<a class="headerlink" href="#diminishing-reward" title="Permalink to this headline">¶</a></h1>
<p>Previous chapters showed that the application to the real-valued signal handling either by using the interval predicates or discretization significantly increases the input-space. This magnitude of possible states implies the growth of classifier population size, therefore the reward distribution using the traditional discounted sum of reward is not an appropriate option. The desired intention is for the case when the rewards received in all decision instances are equally important.</p>
<p>The criterion applied in this situation is called <em>the average reward criterion</em> and was introduced by Puterman <span id="id1">[<a class="reference internal" href="../../bibliography.html#id103">103</a>]</span>. He stated that the decision maker might prefer it when the decisions are made frequently (so that the discount rate is very close to 1) or other terms cannot easily describe the performance criterion. Possible areas of an application might include situations where system performance is assessed based on the throughput rate (like making frequent decisions when controlling the flow of communication networks).</p>
<p>The averaged reward criterion was first implemented in XCS by Tharakunnel and Goldberg <span id="id2">[<a class="reference internal" href="../../bibliography.html#id104">104</a>]</span>. They called their modification AXCS and showed that it performed similarly to the standard XCS in the Woods2 environment. Later, Zang et al. <span id="id3">[<a class="reference internal" href="../../bibliography.html#id105">105</a>]</span> formally introduced the R-learning <span id="id4">[<a class="reference internal" href="../../bibliography.html#id106">106</a>]</span> <span id="id5">[<a class="reference internal" href="../../bibliography.html#id107">107</a>]</span> technique to XCS and called it XCSAR. They compared it with XCSG (where the prediction parameters are modified by applying the idea of gradient descent) and ACXS (maximizing the average of successive rewards) in large multistep problems (Woods1, Maze6, and Woods14).</p>
<p>In this chapter the ACS2 credit assignment component, which is optimizing the performance in the infinite horizon (discounted reward) is replaced with an averaged version. The introduced variant is named AACS2 (Averaged Anticipatory Classifier System) and is implemented in two slightly different variants - AACS2-v1 and AACS2-v2. The performance is validated using two scalable and discretized environments requiring performing multiple steps in pursuance of reward.</p>
<div class="section" id="reinforcement-learning-and-reward-criterion">
<h2>Reinforcement Learning and Reward Criterion<a class="headerlink" href="#reinforcement-learning-and-reward-criterion" title="Permalink to this headline">¶</a></h2>
<p>Reinforcement Learning (RL) is a formal framework in which the agent can influence the environment by executing specific actions and receive corresponding feedback (reward) afterwards. Usually, it takes multiple steps to reach the goal, which makes the process much more complicated. In the general form, RL consists of:</p>
<ul class="simple">
<li><p>A discrete set of environment states <span class="math notranslate nohighlight">\(S\)</span>,</p></li>
<li><p>A discrete set of available actions <span class="math notranslate nohighlight">\(A\)</span>,</p></li>
<li><p>mapping <span class="math notranslate nohighlight">\(R\)</span> between a particular state <span class="math notranslate nohighlight">\(s \in S\)</span> and action <span class="math notranslate nohighlight">\(a \in A\)</span>. The environmental payoff <span class="math notranslate nohighlight">\(r \in R\)</span> describes the expected reward obtained after executing an action in a given state</p></li>
</ul>
<p>In each trial, the agent perceives the environmental state <span class="math notranslate nohighlight">\(s\)</span>. Next, it evaluates all possible actions from <span class="math notranslate nohighlight">\(A\)</span> and executes action <span class="math notranslate nohighlight">\(a\)</span> in the environment. The environment returns a signal <span class="math notranslate nohighlight">\(r\)</span> and next state <span class="math notranslate nohighlight">\(s'\)</span> as intermediate feedback.</p>
<p>The agent’s task is to represent the knowledge, using the policy <span class="math notranslate nohighlight">\(\pi\)</span> mapping states to actions, therefore optimizing a long-run measure of reinforcement. There are two popular optimality criteria used in Markov Decision Problems (MDP) - a <em>discounted reward</em> and an <em>average reward</em> <span id="id6">[<a class="reference internal" href="../../bibliography.html#id108">108</a>]</span> <span id="id7">[<a class="reference internal" href="../../bibliography.html#id111">109</a>]</span>.</p>
<div class="section" id="discounted-reward-criterion">
<h3>Discounted Reward Criterion<a class="headerlink" href="#discounted-reward-criterion" title="Permalink to this headline">¶</a></h3>
<p>In discounted RL, the future rewards are geometrically discounted according to a discount factor <span class="math notranslate nohighlight">\(\gamma\)</span>, where <span class="math notranslate nohighlight">\(0 \leq \gamma &lt; 1\)</span>. The performance is usually optimized in the infinite horizon <span id="id8">[<a class="reference internal" href="../../bibliography.html#id29">39</a>]</span>:</p>
<div class="math notranslate nohighlight">
\[\lim_{N \to \infty} E^{\pi} \left(\sum_{t=0}^{N-1}\gamma^{t} r_{t}(s)\right)\]</div>
<p>The <span class="math notranslate nohighlight">\(E\)</span> expresses the expected value, <span class="math notranslate nohighlight">\(N\)</span> is the number of time steps, and <span class="math notranslate nohighlight">\(r_t(s)\)</span> is the reward received at time <span class="math notranslate nohighlight">\(t\)</span> starting from state <span class="math notranslate nohighlight">\(s\)</span> under the policy.</p>
</div>
<div class="section" id="undiscounted-averaged-reward-criterion">
<h3>Undiscounted (Averaged) Reward Criterion<a class="headerlink" href="#undiscounted-averaged-reward-criterion" title="Permalink to this headline">¶</a></h3>
<p>The <em>averaged reward criterion</em> <span id="id9">[<a class="reference internal" href="../../bibliography.html#id106">106</a>]</span>, which is the undiscounted RL, is where the agent selects actions maximizing its long-run average reward per step <span class="math notranslate nohighlight">\(\rho(s)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\rho^{\pi}(s) = \lim_{N \to \infty} \frac{E^{\pi} \left(\sum_{t=0}^{N-1} r_{t}(s) \right)}{N}\]</div>
<p>If a policy maximizes the average reward over all states, it is a <em>gain optimal policy</em>. Usually, average reward <span class="math notranslate nohighlight">\(\rho(s)\)</span> can be denoted as <span class="math notranslate nohighlight">\(\rho\)</span>, which is state-independent <span id="id10">[<a class="reference internal" href="../../bibliography.html#id112">110</a>]</span>, formulated as <span class="math notranslate nohighlight">\(\rho^{\pi}(x) = \rho^{\pi}(y) = \rho^{\pi},  \forall x,y \in S\)</span> when the resulting Markov chain with policy <span class="math notranslate nohighlight">\(\pi\)</span> is ergodic (aperiodic and positive recurrent) <span id="id11">[<a class="reference internal" href="../../bibliography.html#id113">111</a>]</span>.</p>
<p>To solve an average reward MDP problem, a stationary policy <span class="math notranslate nohighlight">\(\pi\)</span> maximizing the average reward <span class="math notranslate nohighlight">\(\rho\)</span> needs to be determined. To do so, the <em>average adjusted sum</em> of rewards earned following a policy <span class="math notranslate nohighlight">\(\pi\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[V^{\pi}(s) = E^{\pi} \left( \sum_{t=0}^{N \to \infty} (r_t - \rho^{\pi}) \right)\]</div>
<p>The <span class="math notranslate nohighlight">\(V^{\pi}(s)\)</span> can also be called a <em>bias</em> or <em>relative value</em>. Therefore, the optimal relative value for a state–action pair <span class="math notranslate nohighlight">\((s, a)\)</span> can be written as:</p>
<div class="math notranslate nohighlight" id="equation-51-optimal-eq">
<span class="eqno">(5.1)<a class="headerlink" href="#equation-51-optimal-eq" title="Permalink to this equation">¶</a></span>\[V(s, a) = r^{a} (s, s') - \rho + \max_b V(s', b) \forall s \in S \text{ and } \forall a \in A\]</div>
<p>where <span class="math notranslate nohighlight">\(r^{a} (s, s')\)</span> denotes the immediate reward of action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span> when the next state is <span class="math notranslate nohighlight">\(s'\)</span>, <span class="math notranslate nohighlight">\(\rho\)</span> is the average reward, and <span class="math notranslate nohighlight">\(\max_b V(s', b)\)</span> is the maximum relative value in state <span class="math notranslate nohighlight">\(s'\)</span> among all possible actions <span class="math notranslate nohighlight">\(b\)</span>. Equation <a class="reference internal" href="#equation-51-optimal-eq">(5.1)</a> is also known as the Bellman equation for an average reward MDP <span id="id12">[<a class="reference internal" href="../../bibliography.html#id113">111</a>]</span>.</p>
</div>
</div>
<div class="section" id="integrating-reward-criterions-in-acs2">
<h2>Integrating Reward Criterions in ACS2<a class="headerlink" href="#integrating-reward-criterions-in-acs2" title="Permalink to this headline">¶</a></h2>
<p>Despite the ACS’s <em>latent-learning</em> capabilities, the RL is realized using two classifier metrics-reward <span class="math notranslate nohighlight">\(cl.r\)</span>  and immediate reward <span class="math notranslate nohighlight">\(cl.ir\)</span>. The latter stores the immediate reward predicted to be received after acting in a particular situation and is used mainly for model exploitation where the reinforcement might be propagated internally. The reward parameter <span class="math notranslate nohighlight">\(cl.r\)</span> stores the reward predicted to be obtained in the long run.</p>
<p>For the first version of ACS, Stolzmann proposed a <em>bucket-brigade</em> algorithm to update the classifier’s reward <span class="math notranslate nohighlight">\(r_c\)</span> <span id="id13">[<a class="reference internal" href="../../bibliography.html#id86">61</a>]</span> <span id="id14">[<a class="reference internal" href="../../bibliography.html#id114">112</a>]</span>. Let <span class="math notranslate nohighlight">\(c_t\)</span> be the active classifier at time <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(c_{t+1}\)</span> the active classifier at time <span class="math notranslate nohighlight">\(t+1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}r_{c_{t}}(t+1) = \begin{cases} (1-b_r) \cdot r_{c_{t}}(t) + b_r \cdot r(t+1), &amp; \mbox{if } r(t+1) \neq 0\\ (1-b_r) \cdot r_{c_{t}}(t) + b_r \cdot r_{c_{t+1}}(t), &amp; \mbox{if } r(t+1) = 0 \end{cases}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(b_r \in [0,1]\)</span> is the <em>bid-ratio</em>. The idea is that if there is no environmental reward at time <span class="math notranslate nohighlight">\(t+1\)</span>, then the currently active classifier <span class="math notranslate nohighlight">\(c_{t+1}\)</span> gives a payment of <span class="math notranslate nohighlight">\(b_r \cdot r_{c_{t+1}}(t)\)</span> to the previous active classifier <span class="math notranslate nohighlight">\(c_t\)</span>. If there is an environmental reward <span class="math notranslate nohighlight">\(r(t+1)\)</span>, then <span class="math notranslate nohighlight">\(b_r \cdot r(t+1)\)</span> is given to the previous active classifier <span class="math notranslate nohighlight">\(c_t\)</span>.</p>
<p>Later, Butz adopted the Q-learning idea in ACS2 alongside other modifications <span id="id15">[<a class="reference internal" href="../../bibliography.html#id6">65</a>]</span>. For the agent to learn the optimal behavioral policy, both the reward <span class="math notranslate nohighlight">\(cl.r\)</span> and intermediate reward <span class="math notranslate nohighlight">\(cl.ir\)</span> are continuously updated. To assure maximal Q-value, the quality of a classifier is also considered assuming that the reward converges in common with the anticipation’s accuracy. The following updates are applied to each classifier <span class="math notranslate nohighlight">\(cl\)</span> in action set <span class="math notranslate nohighlight">\([A]\)</span> during every trial:</p>
<div class="math notranslate nohighlight" id="equation-51-acs2-qlearning-eq">
<span class="eqno">(5.2)<a class="headerlink" href="#equation-51-acs2-qlearning-eq" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{array}{lcl}
cl.r &amp; = &amp; cl.r + \beta \left(\phi(t) + \gamma \max \limits_{cl' \in [M](t+1) \land cl'.E \neq \{\#\}^L} (cl'.q \cdot cl'.r) - cl.r \right)\\
cl.ir &amp; = &amp; cl.ir + \beta \left( \phi(t) - cl.ir \right)
\end{array}\end{split}\]</div>
<p>The parameter <span class="math notranslate nohighlight">\(\beta \in [0,1]\)</span> denotes the learning rate and <span class="math notranslate nohighlight">\(\gamma \in [0, 1)\)</span> is the discount factor. With a higher <span class="math notranslate nohighlight">\(\beta\)</span> value, the algorithm takes less care of past encountered cases. On the other hand, <span class="math notranslate nohighlight">\(\gamma\)</span> determines to what extent the reward prediction measure depends on future reward.</p>
<p>Thus, in the original ACS2, the calculation of the discounted reward estimation at a specific time <span class="math notranslate nohighlight">\(t\)</span> is described as <span class="math notranslate nohighlight">\(Q(t)\)</span>, which is part of Equation <a class="reference internal" href="#equation-51-acs2-qlearning-eq">(5.2)</a>:</p>
<div class="math notranslate nohighlight">
\[Q(t) \gets \phi(t) + \gamma \max \limits_{cl' \in [M](t+1) \land cl'.E \neq \{\#\}^L} (cl'.q \cdot cl'.r)\]</div>
<p>The modified ACS2 implementation replacing the discounted reward with the averaged version with the formula <span class="math notranslate nohighlight">\(R(t)\)</span> is defined below - Equation <a class="reference internal" href="#equation-51-acs2-relative-reward-eq">(5.3)</a>:</p>
<div class="math notranslate nohighlight" id="equation-51-acs2-relative-reward-eq">
<span class="eqno">(5.3)<a class="headerlink" href="#equation-51-acs2-relative-reward-eq" title="Permalink to this equation">¶</a></span>\[R(t) = \phi(t) - \rho + \max \limits_{cl' \in [M](t+1) \land cl'.E \neq \{\#\}^L} (cl'.q \cdot cl'.r)\]</div>
<p>The definition above requires an estimate of the average reward <span class="math notranslate nohighlight">\(\rho\)</span>. Equation <a class="reference internal" href="#equation-51-optimal-eq">(5.1)</a> showed that the maximization of the average reward is achieved by maximizing the relative value. The next sections will propose two variants of setting it to use the average reward criterion for internal reward distribution. The altered version is named AACS2, which stands for <em>Averaged ACS2</em>.</p>
<p>As the next operation in both cases, the reward parameter of all classifiers in the current action set <span class="math notranslate nohighlight">\([A]\)</span> is updated using the following formula:</p>
<div class="math notranslate nohighlight">
\[cl.r \gets cl.r + \beta (R - cl.r)\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta\)</span> is the learning rate and <span class="math notranslate nohighlight">\(R\)</span> was defined in Equation <a class="reference internal" href="#equation-51-acs2-relative-reward-eq">(5.3)</a>.</p>
<div class="section" id="aacs2-v1">
<h3>AACS2-v1<a class="headerlink" href="#aacs2-v1" title="Permalink to this headline">¶</a></h3>
<p>The first variant of the AACS2 represents <span class="math notranslate nohighlight">\(\rho\)</span> parameter as the ratio of the total reward received along the path to reward and the average number of steps needed. It is initialized as <span class="math notranslate nohighlight">\(\rho = 0\)</span>, and its update is executed as the first operation in RL using the Widrow-Hoff delta rule - Equation <a class="reference internal" href="#equation-51-rho-update-eq">(5.4)</a>. The update is also restricted to be executed only when the agent chooses the action greedily during the explore phase:</p>
<div class="math notranslate nohighlight" id="equation-51-rho-update-eq">
<span class="eqno">(5.4)<a class="headerlink" href="#equation-51-rho-update-eq" title="Permalink to this equation">¶</a></span>\[\rho \gets \rho + \zeta [\phi - \rho]\]</div>
<p>The <span class="math notranslate nohighlight">\(\zeta\)</span> parameter denotes the learning rate for average reward and is typically set at a very low value. This ensures a nearly constant value of average reward for the update of the reward, which is necessary for the convergence of average reward RL algorithms <span id="id16">[<a class="reference internal" href="../../bibliography.html#id115">113</a>]</span>.</p>
</div>
<div class="section" id="aacs2-v2">
<h3>AACS2-v2<a class="headerlink" href="#aacs2-v2" title="Permalink to this headline">¶</a></h3>
<p>The second version is based on the XCSAR proposition by Zang <span id="id17">[<a class="reference internal" href="../../bibliography.html#id105">105</a>]</span>. The only difference from the AACS2-v1 is that the estimate is also dependent on the maximum classifier fitness calculated from the previous and current match set:</p>
<div class="math notranslate nohighlight" id="equation-51-aacs2v2-rho-update-eq">
<span class="eqno">(5.5)<a class="headerlink" href="#equation-51-aacs2v2-rho-update-eq" title="Permalink to this equation">¶</a></span>\[\rho \gets \rho + \zeta [\phi + \max \limits_{cl \in [M](t) \land cl.E \neq \{\#\}^L} (cl.q \cdot cl.r) -\max \limits_{cl \in [M](t+1) \land cl.E \neq \{\#\}^L} (cl.q \cdot cl.r) -\rho]\]</div>
</div>
</div>
<div class="section" id="experimental-evaluation">
<h2>Experimental evaluation<a class="headerlink" href="#experimental-evaluation" title="Permalink to this headline">¶</a></h2>
<p>This section presents the motivation, goals and set-up of the performed experiments, as well as their results.</p>
<div class="section" id="research-questions">
<h3>Research questions<a class="headerlink" href="#research-questions" title="Permalink to this headline">¶</a></h3>
<p>The conducted research aims to answer the following questions:</p>
<ol class="simple">
<li><p>Can the undiscounted reward criterion be used in discretized multistep environments?</p></li>
<li><p>Does the undiscounted reward criterion result in better environment exploitation?</p></li>
</ol>
</div>
<div class="section" id="goals-of-the-experiments">
<h3>Goals of the experiments<a class="headerlink" href="#goals-of-the-experiments" title="Permalink to this headline">¶</a></h3>
<p>In all the experiments different environments are evaluated by two versions of AACS2 algorithm. To accent the changes benchmarking algorithms such as pure ACS2, Q-Learning and R-Learning are also used. The main performance metrics, such as number of steps to reach reward state and the estimated average reward are collected. Additionally, each available state-action pair from the environment is analyzed for the potential calculated reward.</p>
<div class="admonition-experiment-1-straight-corridor admonition">
<p class="admonition-title"><em>Experiment 1 - Straight Corridor</em></p>
<p>The <a class="reference internal" href="../2_selected_topics/26_envs.html#section-topics-environments-corridor"><span class="std std-ref">Corridor</span></a> environment provides a simple, real-valued and scalable benchmark perfectly suited for the problem of long action chains. The agent is required to repeat the same action certain amount of times unless reaching final reward state.</p>
</div>
<div class="admonition-experiment-2-deceptive-corridor admonition">
<p class="admonition-title"><em>Experiment 2 - Deceptive Corridor</em></p>
<p>The <a class="reference internal" href="../2_selected_topics/26_envs.html#section-topics-environments-fsw"><span class="std std-ref">Finite State Worlds</span></a> environment is an extension to the Corridor. In each state actions alternate - one will bring the agent closer to the reward, the other one is languishing. The forager will have to learn to distinguish every other state and distribute rewards properly.</p>
</div>
</div>
<div class="section" id="experiments">
<h3>Experiments<a class="headerlink" href="#experiments" title="Permalink to this headline">¶</a></h3>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="51_experiment_1.html">Experiment 1 - Straight Corridor</a></li>
<li class="toctree-l1"><a class="reference internal" href="51_experiment_2.html">Experiment 2 - Deceptive Corridor</a></li>
</ul>
</div>
</div>
<div class="section" id="answers-to-research-questions">
<h3>Answers to research questions<a class="headerlink" href="#answers-to-research-questions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="can-the-undiscounted-reward-criterion-be-used-in-discretized-multistep-environments">
<h4>Can the undiscounted reward criterion be used in discretized multistep environments?<a class="headerlink" href="#can-the-undiscounted-reward-criterion-be-used-in-discretized-multistep-environments" title="Permalink to this headline">¶</a></h4>
<p>Concluded experiments shown that the undiscounted reward criterion can be successfully applied to problem requiring long-action chains such as the Corridor of FSW. The new system AACS2 varies only in a way the classifier reward <span class="math notranslate nohighlight">\(cl.r\)</span> metric is calculated. The clear difference between the discounted criterion is visible on the payoff landscapes generated from the testing environments. The AACS2 can produce a distinct payoff-landscape with uniformly spaced payoff levels, which is very similar to the one generated by the R-learning algorithm. When taking a closer look, all algorithms generate step-like payoff-landscape plots, but each particular state-action pairs are more distinguishable when the reward-criterion is used. The explanation of why the agent moves toward the goal at all can be found in Equation <a class="reference internal" href="#equation-51-acs2-relative-reward-eq">(5.3)</a> - it is able to find the next best action by using the best classifiers’ fitness from the next match-set.</p>
<p>In addition, the rate at which the average estimate value <span class="math notranslate nohighlight">\(\rho\)</span> converges is different for AACS2-v1 and AACS2-v2. Figures <a class="reference internal" href="51_experiment_1.html#corridor-fig"><span class="std std-numref">5.1</span></a> and <a class="reference internal" href="51_experiment_2.html#fsw-fig"><span class="std std-numref">5.3</span></a> demonstrate that the AACS2-v2 settles to the final value faster, but also has greater fluctuations. That is caused by the fact that both match sets’ maximum fitness is considered when updating the values. Zang also observed this and proposed that the learning rate <span class="math notranslate nohighlight">\(\zeta\)</span> in Equation <a class="reference internal" href="#equation-51-aacs2v2-rho-update-eq">(5.5)</a> could decay over time <span id="id18">[<a class="reference internal" href="../../bibliography.html#id105">105</a>]</span>:</p>
<div class="math notranslate nohighlight">
\[\zeta \gets \zeta - \frac{\zeta^{max} - \zeta^{min}}{NumOfTrials}\]</div>
<p>where <span class="math notranslate nohighlight">\(\zeta^{max}\)</span> is the initial value of <span class="math notranslate nohighlight">\(\zeta\)</span>, and <span class="math notranslate nohighlight">\(\zeta^{min}\)</span> is the minimum learning rate required. The update should take place at the beginning of each exploration trial.</p>
</div>
<div class="section" id="does-the-undiscounted-reward-criterion-result-in-better-environment-exploitation">
<h4>Does the undiscounted reward criterion result in better environment exploitation?<a class="headerlink" href="#does-the-undiscounted-reward-criterion-result-in-better-environment-exploitation" title="Permalink to this headline">¶</a></h4>
<p>Number of steps during exploitation trials depicted on figures <a class="reference internal" href="51_experiment_1.html#corridor-fig"><span class="std std-numref">5.1</span></a> and <a class="reference internal" href="51_experiment_2.html#fsw-fig"><span class="std std-numref">5.3</span></a> indicate that the agent is able to pick up better actions when the undiscounted criterion is used.</p>
<p>In addition, the fact that the <span class="math notranslate nohighlight">\(\rho\)</span> converged to a slightly    suboptimal value might be caused by the exploration strategy adopted, which was set to the <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy. Because the estimated average reward is updated only when the greedy action is executed, the number of greedy actions to be performed during the exploration trial is uncertain. Moreover, the probability distribution when the agent observes the rewarding state might be too low in order to enable the estimated average reward to reach optimal value. This was observed during the experimentation - the <span class="math notranslate nohighlight">\(\rho\)</span> value was correlated with the <span class="math notranslate nohighlight">\(\epsilon\)</span> parameter.</p>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./chapters/5_diminishing_reward"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="51_introduction.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">5. </span>Optimizing distributing reward through long action chains</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="51_experiment_1.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Experiment 1 - Straight Corridor</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Norbert Kozłowski<br/>
    
        &copy; Copyright 2022.<br/>
      <div class="extra_footer">
        <div>
  <p><a href="https://wit.pwr.edu.pl/en/">Faculty of Computer Science and Telecommunications</a> - <a href="https://dce.pwr.edu.pl/">Department of Computer Engineering</a></p>
  <p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><a property="dct:title" rel="cc:attributionURL" href="https://khozzy.github.io/phd">Real-valued Anticipatory Classifier System</a> by <span property="cc:attributionName">Norbert Kozlowski</span> is licensed under <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a></p>
</div> 

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>